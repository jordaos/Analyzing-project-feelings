org.apache.hadoop.io.WritableComparator	[org.apache.hadoop.io.WritableComparator::org.apache.hadoop.io.DataInputBuffer buffer, org.apache.hadoop.io.WritableComparator::org.apache.hadoop.io.WritableComparable key1, org.apache.hadoop.io.WritableComparator::org.apache.hadoop.io.WritableComparable key2, org.apache.hadoop.io.WritableComparator::compare(byte[], int, int, byte[], int, int):int, org.apache.hadoop.io.WritableComparator::compare(org.apache.hadoop.io.WritableComparable, org.apache.hadoop.io.WritableComparable):int]
org.apache.hadoop.mapred.Task	[org.apache.hadoop.mapred.Task::java.lang.String taskId, org.apache.hadoop.mapred.Task::org.apache.hadoop.util.Progress taskProgress, org.apache.hadoop.mapred.Task::reportProgress(org.apache.hadoop.mapred.TaskUmbilicalProtocol, float):void, org.apache.hadoop.mapred.Task::long nextProgressTime, org.apache.hadoop.mapred.Task::reportProgress(org.apache.hadoop.mapred.TaskUmbilicalProtocol):void, org.apache.hadoop.mapred.Task::done(org.apache.hadoop.mapred.TaskUmbilicalProtocol):void, org.apache.hadoop.mapred.Task::getReporter(org.apache.hadoop.mapred.TaskUmbilicalProtocol, org.apache.hadoop.util.Progress):org.apache.hadoop.mapred.Reporter]
org.apache.hadoop.record.compiler.JType	[org.apache.hadoop.record.compiler.JType::java.lang.String mMethodSuffix, org.apache.hadoop.record.compiler.JType::genJavaWriteMethod(java.lang.String, java.lang.String):java.lang.String, org.apache.hadoop.record.compiler.JType::genJavaReadMethod(java.lang.String, java.lang.String):java.lang.String, org.apache.hadoop.record.compiler.JType::java.lang.String mWrapper, org.apache.hadoop.record.compiler.JType::genJavaReadWrapper(java.lang.String, java.lang.String, boolean):java.lang.String, org.apache.hadoop.record.compiler.JType::java.lang.String mUnwrapMethod, org.apache.hadoop.record.compiler.JType::genJavaWriteWrapper(java.lang.String, java.lang.String):java.lang.String]
org.apache.hadoop.record.compiler.JType	[org.apache.hadoop.record.compiler.JType::java.lang.String mMethodSuffix, org.apache.hadoop.record.compiler.JType::genJavaWriteMethod(java.lang.String, java.lang.String):java.lang.String, org.apache.hadoop.record.compiler.JType::genJavaReadMethod(java.lang.String, java.lang.String):java.lang.String, org.apache.hadoop.record.compiler.JType::java.lang.String mWrapper, org.apache.hadoop.record.compiler.JType::genJavaReadWrapper(java.lang.String, java.lang.String, boolean):java.lang.String]
org.apache.hadoop.record.compiler.JType	[org.apache.hadoop.record.compiler.JType::java.lang.String mMethodSuffix, org.apache.hadoop.record.compiler.JType::genJavaWriteMethod(java.lang.String, java.lang.String):java.lang.String, org.apache.hadoop.record.compiler.JType::genJavaReadMethod(java.lang.String, java.lang.String):java.lang.String]
org.apache.hadoop.record.compiler.JType	[org.apache.hadoop.record.compiler.JType::java.lang.String mJavaName, org.apache.hadoop.record.compiler.JType::genJavaDecl(java.lang.String):java.lang.String, org.apache.hadoop.record.compiler.JType::genJavaConstructorParam(int):java.lang.String, org.apache.hadoop.record.compiler.JType::genJavaGetSet(java.lang.String, int):java.lang.String]
org.apache.hadoop.record.compiler.JType	[org.apache.hadoop.record.compiler.JType::java.lang.String mCppName, org.apache.hadoop.record.compiler.JType::genCppDecl(java.lang.String):java.lang.String, org.apache.hadoop.record.compiler.JType::genCppGetSet(java.lang.String, int):java.lang.String]
org.apache.hadoop.dfs.DFSck.Result	[org.apache.hadoop.dfs.DFSck.Result::java.util.ArrayList missingIds, org.apache.hadoop.dfs.DFSck.Result::isHealthy():boolean, org.apache.hadoop.dfs.DFSck.Result::long missingSize, org.apache.hadoop.dfs.DFSck.Result::addMissing(java.lang.String, long):void]
org.apache.hadoop.dfs.FSDataset	[org.apache.hadoop.dfs.FSDataset::java.io.File tmp, org.apache.hadoop.dfs.FSDataset::getTmpFile(org.apache.hadoop.dfs.Block):java.io.File, org.apache.hadoop.dfs.FSDataset::checkDataDir():void, org.apache.hadoop.dfs.FSDataset::org.apache.hadoop.dfs.FSDataset.FSDir dirTree, org.apache.hadoop.dfs.FSDataset::getBlockReport():org.apache.hadoop.dfs.Block[]]
org.apache.hadoop.dfs.FSNamesystem	[org.apache.hadoop.dfs.FSNamesystem::int maxReplication, org.apache.hadoop.dfs.FSNamesystem::verifyReplication(java.lang.String, short, org.apache.hadoop.io.UTF8):void, org.apache.hadoop.dfs.FSNamesystem::int minReplication]
org.apache.hadoop.dfs.FSNamesystem.Lease	[org.apache.hadoop.dfs.FSNamesystem.Lease::java.util.TreeSet locks, org.apache.hadoop.dfs.FSNamesystem.Lease::java.util.TreeSet creates, org.apache.hadoop.dfs.FSNamesystem.Lease::released(org.apache.hadoop.io.UTF8):void, org.apache.hadoop.dfs.FSNamesystem.Lease::completedCreate(org.apache.hadoop.io.UTF8):boolean]
org.apache.hadoop.dfs.MiniDFSCluster	[org.apache.hadoop.dfs.MiniDFSCluster::org.apache.hadoop.dfs.MiniDFSCluster.NameNodeRunner nameNode, org.apache.hadoop.dfs.MiniDFSCluster::shutdown():void, org.apache.hadoop.dfs.MiniDFSCluster::org.apache.hadoop.dfs.MiniDFSCluster.DataNodeRunner dataNode]
org.apache.hadoop.ipc.Server	[org.apache.hadoop.ipc.Server::java.lang.Class paramClass, org.apache.hadoop.ipc.Server::makeParam():org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Server::org.apache.hadoop.conf.Configuration conf]
org.apache.hadoop.mapred.ReduceTaskRunner	[org.apache.hadoop.mapred.ReduceTaskRunner::org.apache.hadoop.mapred.ReduceTask reduceTask, org.apache.hadoop.mapred.ReduceTaskRunner::long lastPollTime, org.apache.hadoop.mapred.ReduceTaskRunner::queryJobTracker(java.util.List, org.apache.hadoop.mapred.InterTrackerProtocol):org.apache.hadoop.mapred.MapOutputLocation[]]
org.apache.hadoop.mapred.TaskTrackerStatus	[org.apache.hadoop.mapred.TaskTrackerStatus::java.util.Vector taskReports, org.apache.hadoop.mapred.TaskTrackerStatus::countReduceTasks():int, org.apache.hadoop.mapred.TaskTrackerStatus::countMapTasks():int, org.apache.hadoop.mapred.TaskTrackerStatus::taskReports():java.util.Iterator]
org.apache.hadoop.metrics.spi.AbstractMetricsContext	[org.apache.hadoop.metrics.spi.AbstractMetricsContext::org.apache.hadoop.metrics.ContextFactory factory, org.apache.hadoop.metrics.spi.AbstractMetricsContext::java.lang.String contextName, org.apache.hadoop.metrics.spi.AbstractMetricsContext::init(java.lang.String, org.apache.hadoop.metrics.ContextFactory):void]
org.apache.hadoop.metrics.spi.AbstractMetricsContext	[org.apache.hadoop.metrics.spi.AbstractMetricsContext::org.apache.hadoop.metrics.ContextFactory factory, org.apache.hadoop.metrics.spi.AbstractMetricsContext::java.lang.String contextName, org.apache.hadoop.metrics.spi.AbstractMetricsContext::init(java.lang.String, org.apache.hadoop.metrics.ContextFactory):void, org.apache.hadoop.metrics.spi.AbstractMetricsContext::getAttribute(java.lang.String):java.lang.String, org.apache.hadoop.metrics.spi.AbstractMetricsContext::getAttributeTable(java.lang.String):java.util.Map]
org.apache.hadoop.streaming.PipeMapRed	[org.apache.hadoop.streaming.PipeMapRed::java.lang.String taskid_, org.apache.hadoop.streaming.PipeMapRed::setStreamProperties():void, org.apache.hadoop.streaming.PipeMapRed::int reportPortPlusOne_]
org.apache.hadoop.streaming.PipeMapRed	[org.apache.hadoop.streaming.PipeMapRed::org.apache.hadoop.streaming.PipeMapRed.MRErrorThread errThread_, org.apache.hadoop.streaming.PipeMapRed::startOutputThreads(org.apache.hadoop.mapred.OutputCollector, org.apache.hadoop.mapred.Reporter):void, org.apache.hadoop.streaming.PipeMapRed::boolean outputDone_, org.apache.hadoop.streaming.PipeMapRed::boolean errorDone_]
org.apache.hadoop.dfs.DatanodeInfo	[org.apache.hadoop.dfs.DatanodeInfo::java.util.TreeSet blocks, org.apache.hadoop.dfs.DatanodeInfo::updateBlocks(org.apache.hadoop.dfs.Block[]):void, org.apache.hadoop.dfs.DatanodeInfo::getBlocks():org.apache.hadoop.dfs.Block[], org.apache.hadoop.dfs.DatanodeInfo::getBlockIterator():java.util.Iterator]
org.apache.hadoop.dfs.FSDirectory	[org.apache.hadoop.dfs.FSDirectory::java.io.DataOutputStream editlog, org.apache.hadoop.dfs.FSDirectory::close():void, org.apache.hadoop.dfs.FSDirectory::logEdit(byte, org.apache.hadoop.io.Writable, org.apache.hadoop.io.Writable):void]
org.apache.hadoop.dfs.FSDirectory	[org.apache.hadoop.dfs.FSDirectory::java.util.TreeMap activeLocks, org.apache.hadoop.dfs.FSDirectory::obtainLock(org.apache.hadoop.io.UTF8, org.apache.hadoop.io.UTF8, boolean):int, org.apache.hadoop.dfs.FSDirectory::releaseLock(org.apache.hadoop.io.UTF8, org.apache.hadoop.io.UTF8):int]
org.apache.hadoop.dfs.FSDirectory	[org.apache.hadoop.dfs.FSDirectory::org.apache.hadoop.dfs.FSDirectory.INode rootDir, org.apache.hadoop.dfs.FSDirectory::java.util.TreeMap activeBlocks, org.apache.hadoop.dfs.FSDirectory::isValidBlock(org.apache.hadoop.dfs.Block):boolean, org.apache.hadoop.dfs.FSDirectory::getFileByBlock(org.apache.hadoop.dfs.Block):org.apache.hadoop.dfs.FSDirectory.INode, org.apache.hadoop.dfs.FSDirectory::unprotectedAddFile(org.apache.hadoop.io.UTF8, org.apache.hadoop.dfs.FSDirectory.INode):boolean, org.apache.hadoop.dfs.FSDirectory::getFile(org.apache.hadoop.io.UTF8):org.apache.hadoop.dfs.Block[], org.apache.hadoop.dfs.FSDirectory::unprotectedRenameTo(org.apache.hadoop.io.UTF8, org.apache.hadoop.io.UTF8):boolean, org.apache.hadoop.dfs.FSDirectory::unprotectedDelete(org.apache.hadoop.io.UTF8):org.apache.hadoop.dfs.Block[], org.apache.hadoop.dfs.FSDirectory::unprotectedSetReplication(java.lang.String, short, java.util.Vector):org.apache.hadoop.dfs.Block[], org.apache.hadoop.dfs.FSDirectory::getBlockSize(java.lang.String):long, org.apache.hadoop.dfs.FSDirectory::getListing(org.apache.hadoop.io.UTF8):org.apache.hadoop.dfs.DFSFileInfo[], org.apache.hadoop.dfs.FSDirectory::isValidToCreate(org.apache.hadoop.io.UTF8):boolean, org.apache.hadoop.dfs.FSDirectory::isDir(org.apache.hadoop.io.UTF8):boolean, org.apache.hadoop.dfs.FSDirectory::unprotectedMkdir(java.lang.String):org.apache.hadoop.dfs.FSDirectory.INode]
org.apache.hadoop.mapred.JobTracker	[org.apache.hadoop.mapred.JobTracker::org.apache.hadoop.ipc.Server interTrackerServer, org.apache.hadoop.mapred.JobTracker::offerService():void, org.apache.hadoop.mapred.JobTracker::startTracker(org.apache.hadoop.conf.Configuration):void]
org.apache.hadoop.mapred.JobTracker.ExpireLaunchingTasks	[org.apache.hadoop.mapred.JobTracker.ExpireLaunchingTasks::java.util.Map launchingTasks, org.apache.hadoop.mapred.JobTracker.ExpireLaunchingTasks::addNewTask(java.lang.String):void, org.apache.hadoop.mapred.JobTracker.ExpireLaunchingTasks::removeTask(java.lang.String):void]
org.apache.hadoop.mapred.StatusHttpServer	[org.apache.hadoop.mapred.StatusHttpServer::org.mortbay.jetty.Server webServer, org.apache.hadoop.mapred.StatusHttpServer::start():void, org.apache.hadoop.mapred.StatusHttpServer::stop():void]
org.apache.hadoop.mapred.StatusHttpServer	[org.apache.hadoop.mapred.StatusHttpServer::org.mortbay.jetty.servlet.WebApplicationContext webAppContext, org.apache.hadoop.mapred.StatusHttpServer::setAttribute(java.lang.String, java.lang.Object):void, org.apache.hadoop.mapred.StatusHttpServer::getAttribute(java.lang.String):java.lang.Object]
org.apache.hadoop.mapred.StatusHttpServer	[org.apache.hadoop.mapred.StatusHttpServer::org.mortbay.http.SocketListener listener, org.apache.hadoop.mapred.StatusHttpServer::getPort():int, org.apache.hadoop.mapred.StatusHttpServer::setThreads(int, int):void]
org.apache.hadoop.metrics.spi.OutputRecord	[org.apache.hadoop.metrics.spi.OutputRecord::org.apache.hadoop.metrics.spi.AbstractMetricsContext.MetricMap metricMap, org.apache.hadoop.metrics.spi.OutputRecord::getMetricNames():java.util.Set, org.apache.hadoop.metrics.spi.OutputRecord::getMetric(java.lang.String):java.lang.Number]
org.apache.hadoop.metrics.spi.OutputRecord	[org.apache.hadoop.metrics.spi.OutputRecord::org.apache.hadoop.metrics.spi.AbstractMetricsContext.TagMap tagMap, org.apache.hadoop.metrics.spi.OutputRecord::getTagNames():java.util.Set, org.apache.hadoop.metrics.spi.OutputRecord::getTag(java.lang.String):java.lang.Object]
org.apache.hadoop.streaming.JarBuilder	[org.apache.hadoop.streaming.JarBuilder::fileExtension(java.lang.String):java.lang.String, org.apache.hadoop.streaming.JarBuilder::getBasePathInJarOut(java.lang.String):java.lang.String, org.apache.hadoop.streaming.JarBuilder::main(java.lang.String[]):void]
org.apache.hadoop.streaming.StreamJob	[org.apache.hadoop.streaming.StreamJob::org.apache.hadoop.streaming.Environment env_, org.apache.hadoop.streaming.StreamJob::init():void, org.apache.hadoop.streaming.StreamJob::getHadoopClientHome():java.lang.String]
org.apache.hadoop.streaming.StreamJob	[org.apache.hadoop.streaming.StreamJob::boolean mayExit_, org.apache.hadoop.streaming.StreamJob::fail(java.lang.String):void, org.apache.hadoop.streaming.StreamJob::exitUsage(boolean):void, org.apache.hadoop.streaming.StreamJob::validateNameEqValue(java.lang.String):void]
org.apache.hadoop.mapred.TaskInProgress	[org.apache.hadoop.mapred.TaskInProgress::java.lang.String jobFile, org.apache.hadoop.mapred.TaskInProgress::int numMaps, org.apache.hadoop.mapred.TaskInProgress::org.apache.hadoop.mapred.JobTracker jobtracker, org.apache.hadoop.mapred.TaskInProgress::org.apache.hadoop.mapred.FileSplit split, org.apache.hadoop.mapred.TaskInProgress::isMapTask():boolean, org.apache.hadoop.mapred.TaskInProgress::int partition, org.apache.hadoop.mapred.TaskInProgress::makeUniqueString(java.lang.String):java.lang.String, org.apache.hadoop.mapred.TaskInProgress::java.util.TreeSet usableTaskIds, org.apache.hadoop.mapred.TaskInProgress::org.apache.hadoop.mapred.JobConf conf, org.apache.hadoop.mapred.TaskInProgress::java.lang.String[] totalTaskIds, org.apache.hadoop.mapred.TaskInProgress::long startTime, org.apache.hadoop.mapred.TaskInProgress::boolean runSpeculative, org.apache.hadoop.mapred.TaskInProgress::hasSpeculativeTask(double):boolean, org.apache.hadoop.mapred.TaskInProgress::double progress, org.apache.hadoop.mapred.TaskInProgress::java.lang.String state, org.apache.hadoop.mapred.TaskInProgress::generateSingleReport():org.apache.hadoop.mapred.TaskReport, org.apache.hadoop.mapred.TaskInProgress::java.util.TreeMap taskDiagnosticData, org.apache.hadoop.mapred.TaskInProgress::updateStatus(org.apache.hadoop.mapred.TaskStatus):void, org.apache.hadoop.mapred.TaskInProgress::int completes, org.apache.hadoop.mapred.TaskInProgress::isComplete():boolean, org.apache.hadoop.mapred.TaskInProgress::boolean failed, org.apache.hadoop.mapred.TaskInProgress::kill():void, org.apache.hadoop.mapred.TaskInProgress::isComplete(java.lang.String):boolean, org.apache.hadoop.mapred.TaskInProgress::completed(java.lang.String):void, org.apache.hadoop.mapred.TaskInProgress::failedSubTask(java.lang.String, java.lang.String):void, org.apache.hadoop.mapred.TaskInProgress::recomputeProgress():void, org.apache.hadoop.mapred.TaskInProgress::hasTask():boolean, org.apache.hadoop.mapred.TaskInProgress::java.util.TreeMap taskStatuses, org.apache.hadoop.mapred.TaskInProgress::java.util.TreeSet recentTasks, org.apache.hadoop.mapred.TaskInProgress::isRunning():boolean, org.apache.hadoop.mapred.TaskInProgress::int numTaskFailures, org.apache.hadoop.mapred.TaskInProgress::java.util.TreeSet machinesWhereFailed, org.apache.hadoop.mapred.TaskInProgress::hasFailedOnMachine(java.lang.String):boolean, org.apache.hadoop.mapred.TaskInProgress::init(java.lang.String):void, org.apache.hadoop.mapred.TaskInProgress::getTaskStatuses():org.apache.hadoop.mapred.TaskStatus[], org.apache.hadoop.mapred.TaskInProgress::java.lang.String id, org.apache.hadoop.mapred.TaskInProgress::getTaskToRun(java.lang.String, org.apache.hadoop.mapred.TaskTrackerStatus, double):org.apache.hadoop.mapred.Task]
org.apache.hadoop.mapred.TaskInProgress	[org.apache.hadoop.mapred.TaskInProgress::org.apache.hadoop.mapred.JobInProgress job, org.apache.hadoop.mapred.TaskInProgress::java.util.TreeSet tasksReportedClosed, org.apache.hadoop.mapred.TaskInProgress::shouldCloseForClosedJob(java.lang.String):boolean]
org.apache.hadoop.dfs.DFSck	[org.apache.hadoop.dfs.DFSck::org.apache.hadoop.dfs.DFSClient dfs, org.apache.hadoop.dfs.DFSck::fsck(java.lang.String):org.apache.hadoop.dfs.DFSck.Result, org.apache.hadoop.dfs.DFSck::org.apache.hadoop.io.UTF8 lostFound, org.apache.hadoop.dfs.DFSck::boolean lfInited, org.apache.hadoop.dfs.DFSck::boolean lfInitedOk, org.apache.hadoop.dfs.DFSck::lostFoundMove(org.apache.hadoop.dfs.DFSFileInfo, org.apache.hadoop.dfs.LocatedBlock[]):void, org.apache.hadoop.dfs.DFSck::lostFoundInit():void, org.apache.hadoop.dfs.DFSck::java.util.Random r, org.apache.hadoop.dfs.DFSck::bestNode(org.apache.hadoop.dfs.DatanodeInfo[], java.util.TreeSet):org.apache.hadoop.dfs.DatanodeInfo, org.apache.hadoop.dfs.DFSck::copyBlock(org.apache.hadoop.dfs.LocatedBlock, org.apache.hadoop.fs.FSOutputStream):void]
org.apache.hadoop.dfs.DFSck	[org.apache.hadoop.dfs.DFSck::org.apache.hadoop.dfs.DFSClient dfs, org.apache.hadoop.dfs.DFSck::fsck(java.lang.String):org.apache.hadoop.dfs.DFSck.Result, org.apache.hadoop.dfs.DFSck::org.apache.hadoop.io.UTF8 lostFound, org.apache.hadoop.dfs.DFSck::boolean lfInited, org.apache.hadoop.dfs.DFSck::boolean lfInitedOk, org.apache.hadoop.dfs.DFSck::lostFoundMove(org.apache.hadoop.dfs.DFSFileInfo, org.apache.hadoop.dfs.LocatedBlock[]):void, org.apache.hadoop.dfs.DFSck::lostFoundInit():void]
org.apache.hadoop.fs.DF	[org.apache.hadoop.fs.DF::java.lang.String filesystem, org.apache.hadoop.fs.DF::getFilesystem():java.lang.String, org.apache.hadoop.fs.DF::long capacity, org.apache.hadoop.fs.DF::getCapacity():long, org.apache.hadoop.fs.DF::long used, org.apache.hadoop.fs.DF::getUsed():long, org.apache.hadoop.fs.DF::long available, org.apache.hadoop.fs.DF::getAvailable():long, org.apache.hadoop.fs.DF::int percentUsed, org.apache.hadoop.fs.DF::getPercentUsed():int, org.apache.hadoop.fs.DF::java.lang.String mount, org.apache.hadoop.fs.DF::getMount():java.lang.String]
org.apache.hadoop.fs.DF	[org.apache.hadoop.fs.DF::java.lang.String filesystem, org.apache.hadoop.fs.DF::getFilesystem():java.lang.String, org.apache.hadoop.fs.DF::long capacity, org.apache.hadoop.fs.DF::getCapacity():long, org.apache.hadoop.fs.DF::long used, org.apache.hadoop.fs.DF::getUsed():long, org.apache.hadoop.fs.DF::long available, org.apache.hadoop.fs.DF::getAvailable():long, org.apache.hadoop.fs.DF::int percentUsed, org.apache.hadoop.fs.DF::getPercentUsed():int]
org.apache.hadoop.fs.DF	[org.apache.hadoop.fs.DF::java.lang.String filesystem, org.apache.hadoop.fs.DF::getFilesystem():java.lang.String, org.apache.hadoop.fs.DF::long capacity, org.apache.hadoop.fs.DF::getCapacity():long, org.apache.hadoop.fs.DF::long used, org.apache.hadoop.fs.DF::getUsed():long, org.apache.hadoop.fs.DF::long available, org.apache.hadoop.fs.DF::getAvailable():long]
org.apache.hadoop.fs.DF	[org.apache.hadoop.fs.DF::java.lang.String filesystem, org.apache.hadoop.fs.DF::getFilesystem():java.lang.String, org.apache.hadoop.fs.DF::long capacity, org.apache.hadoop.fs.DF::getCapacity():long, org.apache.hadoop.fs.DF::long used, org.apache.hadoop.fs.DF::getUsed():long]
org.apache.hadoop.fs.DF	[org.apache.hadoop.fs.DF::java.lang.String filesystem, org.apache.hadoop.fs.DF::getFilesystem():java.lang.String, org.apache.hadoop.fs.DF::long capacity, org.apache.hadoop.fs.DF::getCapacity():long]
org.apache.hadoop.fs.DF	[org.apache.hadoop.fs.DF::long dfInterval, org.apache.hadoop.fs.DF::long lastDF, org.apache.hadoop.fs.DF::doDF():void, org.apache.hadoop.fs.DF::java.lang.String filesystem, org.apache.hadoop.fs.DF::getFilesystem():java.lang.String, org.apache.hadoop.fs.DF::long capacity, org.apache.hadoop.fs.DF::getCapacity():long, org.apache.hadoop.fs.DF::long used, org.apache.hadoop.fs.DF::getUsed():long, org.apache.hadoop.fs.DF::long available, org.apache.hadoop.fs.DF::getAvailable():long, org.apache.hadoop.fs.DF::int percentUsed, org.apache.hadoop.fs.DF::getPercentUsed():int, org.apache.hadoop.fs.DF::java.lang.String mount, org.apache.hadoop.fs.DF::getMount():java.lang.String]
org.apache.hadoop.util.Progress	[org.apache.hadoop.util.Progress::float progress, org.apache.hadoop.util.Progress::int currentPhase, org.apache.hadoop.util.Progress::java.util.ArrayList phases, org.apache.hadoop.util.Progress::phase():org.apache.hadoop.util.Progress, org.apache.hadoop.util.Progress::toString(java.lang.StringBuffer):void, org.apache.hadoop.util.Progress::getInternal():float, org.apache.hadoop.util.Progress::startNextPhase():void, org.apache.hadoop.util.Progress::float progressPerPhase, org.apache.hadoop.util.Progress::addPhase():org.apache.hadoop.util.Progress, org.apache.hadoop.util.Progress::org.apache.hadoop.util.Progress parent, org.apache.hadoop.util.Progress::get():float, org.apache.hadoop.util.Progress::complete():void]
org.apache.hadoop.util.Progress	[org.apache.hadoop.util.Progress::int currentPhase, org.apache.hadoop.util.Progress::java.util.ArrayList phases, org.apache.hadoop.util.Progress::phase():org.apache.hadoop.util.Progress, org.apache.hadoop.util.Progress::toString(java.lang.StringBuffer):void, org.apache.hadoop.util.Progress::getInternal():float, org.apache.hadoop.util.Progress::startNextPhase():void, org.apache.hadoop.util.Progress::float progressPerPhase, org.apache.hadoop.util.Progress::addPhase():org.apache.hadoop.util.Progress]
org.apache.hadoop.conf.Configuration	[org.apache.hadoop.conf.Configuration::getObject(java.lang.String):java.lang.Object, org.apache.hadoop.conf.Configuration::setObject(java.lang.String, java.lang.Object):void, org.apache.hadoop.conf.Configuration::get(java.lang.String, java.lang.Object):java.lang.Object, org.apache.hadoop.conf.Configuration::get(java.lang.String):java.lang.String, org.apache.hadoop.conf.Configuration::set(java.lang.String, java.lang.Object):void, org.apache.hadoop.conf.Configuration::get(java.lang.String, java.lang.String):java.lang.String, org.apache.hadoop.conf.Configuration::getInt(java.lang.String, int):int, org.apache.hadoop.conf.Configuration::getLong(java.lang.String, long):long, org.apache.hadoop.conf.Configuration::getFloat(java.lang.String, float):float, org.apache.hadoop.conf.Configuration::getBoolean(java.lang.String, boolean):boolean, org.apache.hadoop.conf.Configuration::getStrings(java.lang.String):java.lang.String[], org.apache.hadoop.conf.Configuration::setClass(java.lang.String, java.lang.Class, java.lang.Class):void, org.apache.hadoop.conf.Configuration::getFile(java.lang.String, java.lang.String):java.io.File, org.apache.hadoop.conf.Configuration::write(java.io.OutputStream):void, org.apache.hadoop.conf.Configuration::main(java.lang.String[]):void]
org.apache.hadoop.conf.Configuration	[org.apache.hadoop.conf.Configuration::java.lang.ClassLoader classLoader, org.apache.hadoop.conf.Configuration::getResource(java.lang.String):java.net.URL, org.apache.hadoop.conf.Configuration::getClass(java.lang.String, java.lang.Class):java.lang.Class, org.apache.hadoop.conf.Configuration::getConfResourceAsInputStream(java.lang.String):java.io.InputStream, org.apache.hadoop.conf.Configuration::getConfResourceAsReader(java.lang.String):java.io.Reader]
org.apache.hadoop.dfs.DFSClient.DFSInputStream	[org.apache.hadoop.dfs.DFSClient.DFSInputStream::java.lang.String src, org.apache.hadoop.dfs.DFSClient.DFSInputStream::org.apache.hadoop.dfs.Block[] blocks, org.apache.hadoop.dfs.DFSClient.DFSInputStream::org.apache.hadoop.dfs.DatanodeInfo[][] nodes, org.apache.hadoop.dfs.DFSClient.DFSInputStream::openInfo():void]
org.apache.hadoop.dfs.DFSShell	[org.apache.hadoop.dfs.DFSShell::cat(java.lang.String):void, org.apache.hadoop.dfs.DFSShell::copy(java.lang.String, java.lang.String, org.apache.hadoop.conf.Configuration):void, org.apache.hadoop.dfs.DFSShell::setReplication(short, org.apache.hadoop.fs.Path, boolean):void, org.apache.hadoop.dfs.DFSShell::setFileReplication(org.apache.hadoop.fs.Path, short):void, org.apache.hadoop.dfs.DFSShell::ls(java.lang.String, boolean):void, org.apache.hadoop.dfs.DFSShell::du(java.lang.String):void, org.apache.hadoop.dfs.DFSShell::mkdir(java.lang.String):void, org.apache.hadoop.dfs.DFSShell::rename(java.lang.String, java.lang.String):void, org.apache.hadoop.dfs.DFSShell::delete(java.lang.String):void]
org.apache.hadoop.dfs.DFSShell	[org.apache.hadoop.dfs.DFSShell::setReplication(short, org.apache.hadoop.fs.Path, boolean):void, org.apache.hadoop.dfs.DFSShell::setFileReplication(org.apache.hadoop.fs.Path, short):void, org.apache.hadoop.dfs.DFSShell::ls(java.lang.String, boolean):void, org.apache.hadoop.dfs.DFSShell::du(java.lang.String):void]
org.apache.hadoop.streaming.StreamBaseRecordReader	[org.apache.hadoop.streaming.StreamBaseRecordReader::org.apache.hadoop.mapred.Reporter reporter_, org.apache.hadoop.streaming.StreamBaseRecordReader::int numRec_, org.apache.hadoop.streaming.StreamBaseRecordReader::numRecStats(java.lang.CharSequence):void, org.apache.hadoop.streaming.StreamBaseRecordReader::int nextStatusRec_]
org.apache.hadoop.dfs.NameNode	[org.apache.hadoop.dfs.NameNode::org.apache.hadoop.ipc.Server server, org.apache.hadoop.dfs.NameNode::join():void, org.apache.hadoop.dfs.NameNode::boolean stopRequested, org.apache.hadoop.dfs.NameNode::stop():void, org.apache.hadoop.dfs.NameNode::main(java.lang.String[]):void]
org.apache.hadoop.record.compiler.JFile	[org.apache.hadoop.record.compiler.JFile::java.util.ArrayList mInclFiles, org.apache.hadoop.record.compiler.JFile::java.util.ArrayList mRecords, org.apache.hadoop.record.compiler.JFile::genCode(java.lang.String):void]
org.apache.hadoop.streaming.StreamXmlRecordReader	[org.apache.hadoop.streaming.StreamXmlRecordReader::java.lang.String beginMark_, org.apache.hadoop.streaming.StreamXmlRecordReader::java.util.regex.Pattern beginPat_, org.apache.hadoop.streaming.StreamXmlRecordReader::readUntilMatchBegin():boolean, org.apache.hadoop.streaming.StreamXmlRecordReader::readUntilMatchEnd(java.lang.StringBuffer):boolean, org.apache.hadoop.streaming.StreamXmlRecordReader::java.lang.String endMark_, org.apache.hadoop.streaming.StreamXmlRecordReader::java.util.regex.Pattern endPat_, org.apache.hadoop.streaming.StreamXmlRecordReader::boolean slowMatch_]
org.apache.hadoop.streaming.StreamXmlRecordReader	[org.apache.hadoop.streaming.StreamXmlRecordReader::int lookAhead_, org.apache.hadoop.streaming.StreamXmlRecordReader::int maxRecSize_, org.apache.hadoop.streaming.StreamXmlRecordReader::int firstMatchStart_, org.apache.hadoop.streaming.StreamXmlRecordReader::int firstMatchEnd_, org.apache.hadoop.streaming.StreamXmlRecordReader::boolean synched_, org.apache.hadoop.streaming.StreamXmlRecordReader::slowReadUntilMatch(java.util.regex.Pattern, boolean, java.lang.StringBuffer):boolean, org.apache.hadoop.streaming.StreamXmlRecordReader::fastReadUntilMatch(java.lang.String, boolean, java.lang.StringBuffer):boolean]
