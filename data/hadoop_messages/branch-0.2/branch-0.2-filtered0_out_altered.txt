Positive	Negative	Text	Explanation
1	-1	The old core branches are now common branches.    	The old core branches are now common branches .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Moving branches to new location.    	Moving branches to new location .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Merged fixes from trunk, preparing for 0.2.1 release.   	Merged fixes[3]from trunk ,preparing for 0 .[sentence: 3,-1] 2 .[sentence: 1,-1] 1 release .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Preparing to tag 0.1.0 release.   	Preparing to tag 0 .[sentence: 1,-1] 1 .[sentence: 1,-1] 0 release .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Adding branch for 0.2 releases.  2nd attempt.   	Adding branch for 0 .[sentence: 1,-1] 2 releases .[sentence: 1,-1] 2nd attempt .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Quote HADOOP_HOME so that it may contain spaces.    	Quote HADOOP _HOME so that it may contain spaces .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	HADOOP-96.  Logging improvements.  Contributed by Hairong Kuang.   	HADOOP -96 .[sentence: 1,-1] Logging improvements[2].[sentence: 2,-1] Contributed[2]by Hairong Kuang .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-2	HADOOP-198.  Add more example programs to example driver.  Also fix another WritableFactories-related error.  Contributed by Mahadev.   	HADOOP -198 .[sentence: 1,-1] Add more example programs to example driver .[sentence: 1,-1] Also fix[3]another WritableFactories -related error[-2].[sentence: 3,-2] Contributed[2]by Mahadev .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-2	HADOOP-182.  Fix problems related to lost task trackers.   	HADOOP -182 .[sentence: 1,-1] Fix[3]problems[-2]related to lost task trackers .[sentence: 3,-2]  [result: max + and - of any sentence]
2	-1	HADOOP-193 & HADOOP-194.  A filesystem benchmark and a filesystem checker.  Contributed by Konstantin.   	HADOOP -193 &HADOOP -194 .[sentence: 1,-1] A filesystem benchmark and a filesystem checker .[sentence: 1,-1] Contributed[2]by Konstantin .[sentence: 2,-1]  [result: max + and - of any sentence]
2	-1	HADOOP-65.  Initial version of multi-language record system.  Contributed by Milind Bhandarkar.   	HADOOP -65 .[sentence: 1,-1] Initial version of multi -language record system .[sentence: 1,-1] Contributed[2]by Milind Bhandarkar .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-2	Fix a problem introduced by the patch for HADOOP-184, where the 'package' target was broken.  Contributed by Mahadev.   	Fix[3]a problem[-2]introduced by the patch for HADOOP -184 ,where the 'package 'target was broken .[sentence: 3,-2] Contributed[2]by Mahadev .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Better fix for HADOOP-189.  Tested in both distributed and standalone mode.  Note that WritableFactories is not quite functioning correctly and there are a few hacks around this that should be removed when that's better understood.   	Better fix[3]for HADOOP -189 .[sentence: 3,-1] Tested in both distributed and standalone mode .[sentence: 1,-1] Note that WritableFactories is not quite functioning correctly and there are a few hacks around this that should be removed when that's better understood .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Reverting change 399426, which broke distributed operation.   	Reverting change 399426 ,which broke distributed operation .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	HADOOP-191.  Add streaming contrib package.  Contributed by Michel Tourn.   	HADOOP -191 .[sentence: 1,-1] Add streaming contrib package .[sentence: 1,-1] Contributed[2]by Michel Tourn .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-3	HADOOP-192.  Fix a Java 1.4 incompatibility.  Contributed by David Bowen.   	HADOOP -192 .[sentence: 1,-1] Fix[3]a Java 1 .[sentence: 3,-1] 4 incompatibility[-3].[sentence: 1,-3] Contributed[2]by David Bowen .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	HADOOP-189.  Fix MapReduce in standalone configuration to correctly handle job jar files that contain a lib directory with nested jar files.   	HADOOP -189 .[sentence: 1,-1] Fix[3]MapReduce in standalone configuration to correctly handle job jar files that contain a lib directory with nested jar files .[sentence: 3,-1]  [result: max + and - of any sentence]
2	-1	HADOOP-184.  Restructure some test code to better support testing on a cluster.  Contributed by Mahadev.   	HADOOP -184 .[sentence: 1,-1] Restructure some test code to better support testing on a cluster .[sentence: 1,-1] Contributed[2]by Mahadev .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Fix HADOOP-19.  If a child process hangs after it has reported completion its output should not be lost.   	Fix[3]HADOOP -19 .[sentence: 3,-1] If a child process hangs after it has reported completion its output should not be lost .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	HADOOP-185.  Fix so that, if a task tracker times out making the RPC asking for a new task to run, the job tracker does not think that it is actually running the task returned (but never received).  Contributed by Owen.   	HADOOP -185 .[sentence: 1,-1] Fix[3]so that ,if a task tracker times out making the RPC asking for a new task to run ,the job tracker does not think that it is actually running the task returned (but never received ).[sentence: 3,-1] Contributed[2]by Owen .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	HADOOP-188.  More fixes to to JobClient, following on HADOOP-174.  Contributed by Owen.   	HADOOP -188 .[sentence: 1,-1] More fixes[3]to to JobClient ,following on HADOOP -174 .[sentence: 3,-1] Contributed[2]by Owen .[sentence: 2,-1]  [result: max + and - of any sentence]
2	-1	HADOOP-187.  Add RandomWriter and Sort examples.  Contributed by Owen.   	HADOOP -187 .[sentence: 1,-1] Add RandomWriter and Sort examples .[sentence: 1,-1] Contributed[2]by Owen .[sentence: 2,-1]  [result: max + and - of any sentence]
2	-2	HADOOP-186.  Better error handling in TaskTracker's top-level loop.  Also improve calculation of time to send next heartbeat.  Contributed by Owen O'Malley.   	HADOOP -186 .[sentence: 1,-1] Better error[-2]handling in TaskTracker's top -level loop .[sentence: 1,-2] Also improve[2]calculation of time to send next heartbeat .[sentence: 2,-1] Contributed[2]by Owen O'Malley .[sentence: 2,-1]  [result: max + and - of any sentence]
2	-1	HADOOP-183.  If the namendode is restarted with different minimum or maximum replication counts, existing files' replication counts are now automatically adjusted to be within the newly configured bounds.  Contributed by Hairong Kuang.   	HADOOP -183 .[sentence: 1,-1] If the namendode is restarted with different minimum or maximum replication counts ,existing files 'replication counts are now automatically adjusted to be within the newly configured bounds .[sentence: 1,-1] Contributed[2]by Hairong Kuang .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-2	HADOOP-178.  Piggyback DFS blockwork requests on heartbeat responses, reducing traffic.  Also move blockwork delay on startup from datanode to namenode, fixing a problems when the namenode alone restarts.  Contributed by Hairong Kuang.   	HADOOP -178 .[sentence: 1,-1] Piggyback DFS blockwork requests on heartbeat responses ,reducing traffic .[sentence: 1,-1] Also move blockwork delay[-2]on startup from datanode to namenode ,fixing[3]a problems[-2]when the namenode alone restarts .[sentence: 3,-2] Contributed[2]by Hairong Kuang .[sentence: 2,-1]  [result: max + and - of any sentence]
1	-1	HADOOP-177.  Page through tasks in web ui.   	HADOOP -177 .[sentence: 1,-1] Page through tasks in web ui .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fix HADOOP-174.  Make job client try up to five times to contact job tracker before aborting a job.  Contributed by Owen.   	Fix[3]HADOOP -174 .[sentence: 3,-1] Make job client try up to five times to contact job tracker before aborting a job .[sentence: 1,-1] Contributed[2]by Owen .[sentence: 2,-1]  [result: max + and - of any sentence]
1	-1	Note recent changes.   	Note recent changes .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-2	Fix createNewFile() so that it creates a .crc file, like everything else does.  This fixes some annoyances in Nutch.    	Fix[3]createNewFile ()so that it creates a .[sentence: 3,-1] crc file ,like everything else does .[sentence: 1,-1] This fixes[3]some annoyances[-3][--1 booster word]in Nutch .[sentence: 3,-2]  [result: max + and - of any sentence]
3	-1	Fix HADOOP-167.  Reduce the number of Configuration and JobConf's allocated.  Contributed by Owen.   	Fix[3]HADOOP -167 .[sentence: 3,-1] Reduce the number of Configuration and JobConf's allocated .[sentence: 1,-1] Contributed[2]by Owen .[sentence: 2,-1]  [result: max + and - of any sentence]
1	-1	Change default replication.max in code to be the same as in hadoop-default.xml.    	Change default replication .[sentence: 1,-1] max in code to be the same as in hadoop -default .[sentence: 1,-1] xml .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	HADOOP-173.  Optimize allocation of tasks with local data.   	HADOOP -173 .[sentence: 1,-1] Optimize allocation of tasks with local data .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Permit configuration to specify higher replication for job submission files.  Also reduce complaints when a file's replication is greater than the size of the cluster.   	Permit configuration to specify higher replication for job submission files .[sentence: 1,-1] Also reduce complaints[-2]when a file's replication is greater than the size of the cluster .[sentence: 1,-2]  [result: max + and - of any sentence]
3	-1	Fix bug introduced yesterday.  NullInstance really is still required!   	Fix[3]bug introduced yesterday .[sentence: 3,-1] NullInstance really is still required ![+0.6 punctuation mood emphasis][sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Fix HADOOP-170.  Permit FileSystem clients to examine and modify the replication count of individual files.  Also fix a few replication-related bugs.  Contributed by Konstantin Shvachko.   	Fix[3]HADOOP -170 .[sentence: 3,-1] Permit FileSystem clients to examine and modify the replication count of individual files .[sentence: 1,-1] Also fix[3]a few replication -related bugs .[sentence: 3,-1] Contributed[2]by Konstantin Shvachko .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Fix HADOOP-169.  Don't fail reduce tasks if a call to the jobtracker to locate map outputs fails.  Contributed by Owen.   	Fix[3]HADOOP -169 .[sentence: 3,-1] Don't fail reduce tasks if a call to the jobtracker to locate map outputs fails .[sentence: 1,-1] Contributed[2]by Owen .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Fix HADOOP-168.  Add IOException to throws of all MapReduce RPC protocol methods.  Contributed by Owen.   	Fix[3]HADOOP -168 .[sentence: 3,-1] Add IOException to throws of all MapReduce RPC protocol methods .[sentence: 1,-1] Contributed[2]by Owen .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-3	Fix HADOOP-166.  RPCs can now pass subclasses of declared types as parameters.  Note this change is incompatible for any application that stores ObjectWritables in a file.  Nutch only stores ObjectWritable in temporary intermediate files, so this is not a problem for Nutch.   	Fix[3]HADOOP -166 .[sentence: 3,-1] RPCs can now pass subclasses of declared types as parameters .[sentence: 1,-1] Note this change is incompatible[-3]for any application that stores ObjectWritables in a file .[sentence: 1,-3] Nutch only stores ObjectWritable in temporary intermediate files ,so this is not a problem[-2]for Nutch .[sentence: 1,-2]  [result: max + and - of any sentence]
1	-1	Remove some Java 1.5 dependencies from the new metrics code.   	Remove some Java 1 .[sentence: 1,-1] 5 dependencies from the new metrics code .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fix HADOOP-160.  Remove some uneeded synchronization around time-consuming operations in the TaskTracker.   	Fix[3]HADOOP -160 .[sentence: 3,-1] Remove some uneeded synchronization around time -consuming operations in the TaskTracker .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Add attribution.    	Add attribution .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fix HADOOP-132.  Add an API for reporting metrics (as yet unused).  Contributed by David Bowen.   	Fix[3]HADOOP -132 .[sentence: 3,-1] Add an API for reporting metrics (as yet unused ).[sentence: 1,-1] Contributed[2]by David Bowen .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Fix HADOOP-162.  Stop generating ConcurrentModificationExceptions when releasing file locks.  Contributed by Owen O'Malley.   	Fix[3]HADOOP -162 .[sentence: 3,-1] Stop generating ConcurrentModificationExceptions when releasing file locks .[sentence: 1,-1] Contributed[2]by Owen O'Malley .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-154 (fixed in other relevant places, too).    	Fix[3]for HADOOP -154 (fixed[3]in other relevant places ,too ).[sentence: 3,-1]  [result: max + and - of any sentence]
3	-1	Fix HADOOP-150.  Improved task names that include job name.   	Fix[3]HADOOP -150 .[sentence: 3,-1] Improved[2]task names that include job name .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-2	Fix HADOOP-157.  Make dfs client wait long enough for locks on abandoned files to expire when creating files, so that when a task that writes to dfs fails, its replacements do not also immediately fail when they try to open the same files.  Contributed by Owen O'Malley.   	Fix[3]HADOOP -157 .[sentence: 3,-1] Make dfs client wait long enough for locks on abandoned[-2]files to expire when creating files ,so that when a task that writes to dfs fails ,its replacements do not also immediately fail when they try to open the same files .[sentence: 1,-2] Contributed[2]by Owen O'Malley .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Fix HADOOP-69.  NPE when getting hints for a non-existant file chunk.  Contributed by Bryan Pendelton.   	Fix[3]HADOOP -69 .[sentence: 3,-1] NPE when getting hints for a non -existant file chunk .[sentence: 1,-1] Contributed[2]by Bryan Pendelton .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-2	Fix HADOOP-151.  Close a potential socket leak.   	Fix[3]HADOOP -151 .[sentence: 3,-1] Close a potential socket leak[-2].[sentence: 1,-2]  [result: max + and - of any sentence]
3	-1	Fix HADOOP-148.  Maintain a task failure count per tasktracker and display it in the web ui.  Contributed by Owen.   	Fix[3]HADOOP -148 .[sentence: 3,-1] Maintain a task failure count per tasktracker and display it in the web ui .[sentence: 1,-1] Contributed[2]by Owen .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-2	Fix for HADOOP-142.  Avoid re-running a task on a node where it has previously failed.  Contributed by Owen.   	Fix[3]for HADOOP -142 .[sentence: 3,-1] Avoid[-2]re -running a task on a node where it has previously failed .[sentence: 1,-2] Contributed[2]by Owen .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-2	Fix for HADOOP-133.  Retry pings from child to parent, in case of (local) communcation problems.  Also log exit status, so that one can distinguish patricide from other deaths.  Contributed by Owen.   	Fix[3]for HADOOP -133 .[sentence: 3,-1] Retry pings from child to parent ,in case of (local )communcation problems[-2].[sentence: 1,-2] Also log exit[-2]status ,so that one can distinguish patricide from other deaths .[sentence: 1,-2] Contributed[2]by Owen .[sentence: 2,-1]  [result: max + and - of any sentence]
1	-1	Log the last two commits.    	Log the last two commits .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-134.  Don't hang jobs when the tasktracker is misconfigured to use an un-writable local directory.  Contributed by Owen.   	Fix[3]for HADOOP -134 .[sentence: 3,-1] Don't hang[-2][=0 negation]jobs when the tasktracker is misconfigured to use an un -writable local directory .[sentence: 1,-1] Contributed[2]by Owen .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-2	Fix HADOOP-114.  Error message named wrong config property.  Contributed by Michael Stack.   	Fix[3]HADOOP -114 .[sentence: 3,-1] Error[-2]message named wrong config property .[sentence: 1,-2] Contributed[2]by Michael Stack .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-2	Fix for HADOOP-139.  Fix a potential deadlock in LocalFileSystem.lock().  Contributed by Igor Bolotin.   	Fix[3]for HADOOP -139 .[sentence: 3,-1] Fix[3]a potential deadlock[-2]in LocalFileSystem .[sentence: 3,-2] lock ().[sentence: 1,-1] Contributed[2]by Igor Bolotin .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Fix HADOOP-138.  Stop multiple tasks per heartbeat.  Contributed by Stefan.   	Fix[3]HADOOP -138 .[sentence: 3,-1] Stop multiple tasks per heartbeat .[sentence: 1,-1] Contributed[2]by Stefan .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-2	Fix HADOOP-118.  Improved cleanup of abandoned file creations in DFS.  Contributed by Owen.   	Fix[3]HADOOP -118 .[sentence: 3,-1] Improved[2]cleanup of abandoned[-2]file creations in DFS .[sentence: 2,-2] Contributed[2]by Owen .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Fix HADOOP-143.  Stop line-wrapping when displaying stack traces.  Contributed by Owen O'Malley.   	Fix[3]HADOOP -143 .[sentence: 3,-1] Stop line -wrapping when displaying stack traces .[sentence: 1,-1] Contributed[2]by Owen O'Malley .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Fix HADOOP-144.  Use mapred task id as dfs client id to faciliate debugging.  Contributed by Owen O'Malley.   	Fix[3]HADOOP -144 .[sentence: 3,-1] Use mapred task id as dfs client id to faciliate debugging .[sentence: 1,-1] Contributed[2]by Owen O'Malley .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Fixed HADOOP-129.  Replaced uses of java.io.File in FileSystem API with a new class named Path.  Also dfs.local.dir and mapred.local.dir may no longer be space-separated, but must now be comma-separated lists of directories.   	Fixed[3]HADOOP -129 .[sentence: 3,-1] Replaced uses of java .[sentence: 1,-1] io .[sentence: 1,-1] File in FileSystem API with a new class named Path .[sentence: 1,-1] Also dfs .[sentence: 1,-1] local .[sentence: 1,-1] dir and mapred .[sentence: 1,-1] local .[sentence: 1,-1] dir may no longer be space -separated ,but must now be comma -separated lists of directories .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Updated change log with recent changes.    	Updated change log with recent changes .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-2	Fix HADOOP-128.  Improved DFS error handling.  Contributed by Owen O'Malley.   	Fix[3]HADOOP -128 .[sentence: 3,-1] Improved[2]DFS error[-2]handling .[sentence: 2,-2] Contributed[2]by Owen O'Malley .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-92.  Show information about all attempts to run each task in the web ui.  Contributed by Mahadev konar.   	Fix[3]for HADOOP -92 .[sentence: 3,-1] Show information about all attempts to run each task in the web ui .[sentence: 1,-1] Contributed[2]by Mahadev konar .[sentence: 2,-1]  [result: max + and - of any sentence]
1	-1	Add some logging during shuffle.    	Add some logging during shuffle .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Add link to store.   	Add link to store .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Stop using ssh options by default that are not yet in widely used versions of ssh.  Folks can still enable their use by uncommenting a line in conf/hadoop-env.sh.   	Stop using ssh options by default that are not yet in widely used versions of ssh .[sentence: 1,-1] Folks can still enable their use by uncommenting a line in conf /hadoop -env .[sentence: 1,-1] sh .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fix HADOOP-131.  Add scripts to start/stop dfs and mapred daemons.  Use these in start/stop-all scripts.  Contributed by Chris Mattmann.   	Fix[3]HADOOP -131 .[sentence: 3,-1] Add scripts to start /stop dfs and mapred daemons .[sentence: 1,-1] Use these in start /stop -all scripts .[sentence: 1,-1] Contributed[2]by Chris Mattmann .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Fix script documentation.  Thanks, Stefan!    	Fix[3]script documentation .[sentence: 3,-1] Thanks[2],Stefan ![+0.6 punctuation mood emphasis][sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-51.  Support per-file replication counts in DFS.  Contributed by Konstantin Shvachko.   	Fix[3]for HADOOP -51 .[sentence: 3,-1] Support per -file replication counts in DFS .[sentence: 1,-1] Contributed[2]by Konstantin Shvachko .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-126.  'bin/hadoop dfs -cp' now correctly handles .crc files.  This also consolidates a lot of file copying code.  Contributed by Konstantin Shvachko.   	Fix[3]for HADOOP -126 .[sentence: 3,-1] 'bin /hadoop dfs -cp 'now correctly handles .[sentence: 1,-1] crc files .[sentence: 1,-1] This also consolidates a lot of file copying code .[sentence: 1,-1] Contributed[2]by Konstantin Shvachko .[sentence: 2,-1]  [result: max + and - of any sentence]
1	-1	Preparing for 0.1.1 release.   	Preparing for 0 .[sentence: 1,-1] 1 .[sentence: 1,-1] 1 release .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-2	Fix for HADOOP-125.  Absolute paths are tricky on Windows.  For Hadoop's purposes, consider things that start with a slash to be absolute.  Also, Hadoop should not change the JVM's CWD.  All files are now correctly cleaned up for a Nutch crawl, in either local or psuedo-distributed mode.   	Fix[3]for HADOOP -125 .[sentence: 3,-1] Absolute paths are tricky[-2]on Windows .[sentence: 1,-2] For Hadoop's purposes ,consider things that start with a slash[-2]to be absolute .[sentence: 1,-2] Also ,Hadoop should not change the JVM's CWD .[sentence: 1,-1] All files are now correctly cleaned up for a Nutch crawl ,in either local or psuedo -distributed mode .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Remove .crc files too.    	Remove .[sentence: 1,-1] crc files too .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fix HADOOP-116.  Clean up job submission files.  On job completion, remove the directory containing the submitted job.xml file, since JobClient always creates a new directory to hold this file.   	Fix[3]HADOOP -116 .[sentence: 3,-1] Clean up job submission files .[sentence: 1,-1] On job completion ,remove the directory containing the submitted job .[sentence: 1,-1] xml file ,since JobClient always creates a new directory to hold this file .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fix HADOOP-117.  Correctly remove mapred temp files.   	Fix[3]HADOOP -117 .[sentence: 3,-1] Correctly remove mapred temp files .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Update change log.    	Update change log .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fix so that close() throws IOException, so that classes which override this method can throw IOException, as specified in the Closeable interface.  Also add the Apache license (which must be in every file) and add a bit more javadoc.    	Fix[3]so that close ()throws IOException ,so that classes which override this method can throw IOException ,as specified in the Closeable interface .[sentence: 3,-1] Also add the Apache license (which must be in every file )and add a bit more javadoc .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Trunk is now 0.2-dev.    	Trunk is now 0 .[sentence: 1,-1] 2 -dev .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Updating the copyright year.    	Updating the copyright year .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	Starting a Hadoop Change Log.  Developers: please add a note to this file for each significant change.  Contributed patches should ideally include an entry for this file.    	Starting a Hadoop Change Log .[sentence: 1,-1] Developers :please[2]add a note to this file for each significant change .[sentence: 2,-1] Contributed[2]patches should ideally include an entry for this file .[sentence: 2,-1]  [result: max + and - of any sentence]
1	-1	Update site for 0.1.0 release.   	Update site for 0 .[sentence: 1,-1] 1 .[sentence: 1,-1] 0 release .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-2	More fixes to get working directory to work on Windows.  Long-term we should stop using java.io.File for our abstract file paths.  On Windows, 'new File(/foo/bar).isAbsolute()' returns false, which caused lots of problems.  So I have added a FileSystem.isAbsolute() method that we use internally.  I also added a few more .getAbsoluteFile() calls to convert paths into absolute paths so that LocalFileSystem works correctly.  Finally I took advantage of file status information cached in DFSFile, eliminating some namenode RPCs.   	More fixes[3]to get working directory to work on Windows .[sentence: 3,-1] Long -term we should stop using java .[sentence: 1,-1] io .[sentence: 1,-1] File for our abstract file paths .[sentence: 1,-1] On Windows ,'new File (/foo /bar ).[sentence: 1,-1] isAbsolute ()'returns false ,which caused lots of problems[-2].[sentence: 1,-2] So I have added a FileSystem .[sentence: 1,-1] isAbsolute ()method that we use internally .[sentence: 1,-1] I also added a few more .[sentence: 1,-1] getAbsoluteFile ()calls to convert paths into absolute paths so that LocalFileSystem works correctly .[sentence: 1,-1] Finally I took advantage of file status information cached in DFSFile ,eliminating some namenode RPCs .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fix wiki url.    	Fix[3]wiki url .[sentence: 3,-1]  [result: max + and - of any sentence]
1	-2	If close() fails, then abandon the file, so that any leases are cleared and other task may attempt to create it.    	If close ()fails ,then abandon[-2]the file ,so that any leases are cleared and other task may attempt to create it .[sentence: 1,-2]  [result: max + and - of any sentence]
3	-2	Fix a bug where writing zero-length files would cause things to hang.    	Fix[3]a bug where writing zero -length files would cause things to hang[-2].[sentence: 3,-2]  [result: max + and - of any sentence]
1	-1	DFS re-format should fully delete old namenode data.   	DFS re -format should fully delete old namenode data .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-102.  Contributed by Konstantin.    	Fix[3]for HADOOP -102 .[sentence: 3,-1] Contributed[2]by Konstantin .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Fix HADOOP-100.  Be more consistent about synchronization of access to taskTracker collection.  Contributed by Owen O'Malley.   	Fix[3]HADOOP -100 .[sentence: 3,-1] Be more consistent about synchronization of access to taskTracker collection .[sentence: 1,-1] Contributed[2]by Owen O'Malley .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-2	Fix for HADOOP-107.  As they were written, dfs blocks were both trickled to a datanode and tee'd to a temp file (in case the connection to the datanode failed).  Now they're only written to the temp file, with no connection to the datanode made until the block is complete.  This reduces the number of long-lived mostly-idle connections to datanodes, which was causing problems.  It also simplifies the DFSClient code significantly.   	Fix[3]for HADOOP -107 .[sentence: 3,-1] As they were written ,dfs blocks were both trickled[-2]to a datanode and tee'd to a temp file (in case the connection to the datanode failed ).[sentence: 1,-2] Now they're only written to the temp file ,with no connection to the datanode made until the block is complete .[sentence: 1,-1] This reduces the number of long -lived mostly -idle connections to datanodes ,which was causing problems[-2].[sentence: 1,-2] It also simplifies the DFSClient code significantly .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-103, part II: I forgot to add this file the first time!   	Fix[3]for HADOOP -103 ,part II :I forgot to add this file the first time ![+0.6 punctuation mood emphasis][sentence: 3,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-103.  Add a base class for Mapper and Reducer implementations that implements Closeable and JobConfigurable.  Use it in supplied Mappers & Reducers.  Also some minor improvements to demos.  Contributed by Owen O'Malley.   	Fix[3]for HADOOP -103 .[sentence: 3,-1] Add a base class for Mapper and Reducer implementations that implements Closeable and JobConfigurable .[sentence: 1,-1] Use it in supplied Mappers &Reducers .[sentence: 1,-1] Also some minor improvements[2]to demos .[sentence: 2,-1] Contributed[2]by Owen O'Malley .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-110.  Reuse keys and values when mapping.  Contributed by Owen O'Malley.    	Fix[3]for HADOOP -110 .[sentence: 3,-1] Reuse keys and values[2]when mapping .[sentence: 2,-1] Contributed[2]by Owen O'Malley .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-2.  The combiner now clones keys and values, so mappers may now safely reuse emitted keys and values.  Contributed by Owen O'Malley.   	Fix[3]for HADOOP -2 .[sentence: 3,-1] The combiner now clones keys and values[2],so mappers may now safely reuse emitted keys and values[2].[sentence: 2,-1] Contributed[2]by Owen O'Malley .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-112.  All operations on local files are now performed through a LocalFileSystem.  In particular, listing the local directory, which was causing this bug, when CRC files were included in the listing.  Now they are correctly excluded.    	Fix[3]for HADOOP -112 .[sentence: 3,-1] All operations on local files are now performed through a LocalFileSystem .[sentence: 1,-1] In particular ,listing the local directory ,which was causing this bug ,when CRC files were included in the listing .[sentence: 1,-1] Now they are correctly excluded .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-2	Fix for HADOOP-84.  Improve error and log messages when block cannot be obtained by including file name and offset.  Also removed a few unused variables.  Contributed by Konstantin Shvachko.   	Fix[3]for HADOOP -84 .[sentence: 3,-1] Improve[2]error[-2]and log messages when block cannot be obtained by including file name and offset .[sentence: 2,-2] Also removed a few unused variables .[sentence: 1,-1] Contributed[2]by Konstantin Shvachko .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-2	Fix HADOOP-33.  Avoid calling df too frequently by caching values internally.  Contributed by Konstantin Shvachko.   	Fix[3]HADOOP -33 .[sentence: 3,-1] Avoid[-2]calling df too frequently by caching values[2]internally .[sentence: 2,-2] Contributed[2]by Konstantin Shvachko .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Fix HADOOP-67.  Add a public API for dfs statistics.  Also switch to use the public API for reporting in DFSShell.   	Fix[3]HADOOP -67 .[sentence: 3,-1] Add a public API for dfs statistics .[sentence: 1,-1] Also switch to use the public API for reporting in DFSShell .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-2	Add a tool for checking DFS consistency (HADOOP-101).  Add a shortcut to bin/hadoop. In accordance with long-standing *nix tradition this command is called 'fsck'.  Development of this tool has been supported by Krugle.net. Thank you!    	Add a tool for checking DFS consistency (HADOOP -101 ).[sentence: 1,-1] Add a shortcut to bin /hadoop .[sentence: 1,-1] In accordance with long -standing *nix[-2]tradition this command is called 'fsck '.[sentence: 1,-2] Development of this tool has been supported[2]by Krugle .[sentence: 2,-1] net .[sentence: 1,-1] Thank[2]you ![+0.6 punctuation mood emphasis][sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Fix for file names with spaces.    	Fix[3]for file names with spaces .[sentence: 3,-1]  [result: max + and - of any sentence]
3	-2	Always return an absolute pathname for local files.  This fixes problems on Windows, where a path specified with "/foo" in a config file is sometimes treated as a relative path.    	Always return an absolute pathname for local files .[sentence: 1,-1] This fixes[3]problems[-2]on Windows ,where a path specified with '/foo 'in a config file is sometimes treated as a relative path .[sentence: 3,-2]  [result: max + and - of any sentence]
3	-2	Fix unit tests on Windows.  Don't assume that, just because a pathname begins with a slash that it also returns true for File.isAbsolute().  Instead use getAbsoluteFile() to force such things to be absolute.   	Fix[3]unit tests on Windows .[sentence: 3,-1] Don't assume that ,just because a pathname begins with a slash[-2]that it also returns true[2]for File .[sentence: 2,-2] isAbsolute ().[sentence: 1,-1] Instead use getAbsoluteFile ()to force such things to be absolute .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Document the new format command.    	Document the new format command .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-3	Fix for HADOOP-19.  A namenode must now be formatted before it may be used.  Attempts to start a namenode in an unformatted directory will fail, rather than automatically creating a new, empty filesystem, causing existing datanodes to delete all blocks.  Thus a mis-configured dfs.data.dir should no longer cause data loss.   	Fix[3]for HADOOP -19 .[sentence: 3,-1] A namenode must now be formatted before it may be used .[sentence: 1,-1] Attempts to start a namenode in an unformatted directory will fail ,rather than automatically creating a new ,empty filesystem ,causing existing datanodes to delete all blocks .[sentence: 1,-1] Thus a mis -configured dfs .[sentence: 1,-1] data .[sentence: 1,-1] dir should no longer cause data loss[-3].[sentence: 1,-3]  [result: max + and - of any sentence]
1	-1	Move checking of output directory existence from JobClient to OutputFormat, so that it can be overridden.  Add a base class for OutputFormat that implements this new method.   	Move checking of output directory existence from JobClient to OutputFormat ,so that it can be overridden .[sentence: 1,-1] Add a base class for OutputFormat that implements this new method .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-46.  Jobs can now be named.  Contributed by Owen O'Malley.   	Fix[3]for HADOOP -46 .[sentence: 3,-1] Jobs can now be named .[sentence: 1,-1] Contributed[2]by Owen O'Malley .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-98.  Keep more accurate task counts.  Contributed by Owen O'Malley.   	Fix[3]for HADOOP -98 .[sentence: 3,-1] Keep more accurate task counts .[sentence: 1,-1] Contributed[2]by Owen O'Malley .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-52.  Add username and working-directory to FileSystem and JobConf and use these to resolve relative paths.  Contributed by Owen O'Malley.   	Fix[3]for HADOOP -52 .[sentence: 3,-1] Add username and working -directory to FileSystem and JobConf and use these to resolve relative paths .[sentence: 1,-1] Contributed[2]by Owen O'Malley .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-83.   	Fix[3]for HADOOP -83 .[sentence: 3,-1]  [result: max + and - of any sentence]
3	-1	Much improved hadoop logo.  Contributed by Stefan.  Thanks!   	Much improved[2]hadoop logo .[sentence: 2,-1] Contributed[2]by Stefan .[sentence: 2,-1] Thanks[2]![+0.6 punctuation emphasis][sentence: 3,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-3.  Don't permit jobs to write to a pre-existing output directory.  Contributed by Owen O'Malley.   	Fix[3]for HADOOP -3 .[sentence: 3,-1] Don't permit jobs to write to a pre -existing output directory .[sentence: 1,-1] Contributed[2]by Owen O'Malley .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-2	Fix for HADOOP-45.  Fatal task errors are now logged at the JobTracker, facilitating debugging.    	Fix[3]for HADOOP -45 .[sentence: 3,-1] Fatal task errors[-2]are now logged at the JobTracker ,facilitating debugging .[sentence: 1,-2]  [result: max + and - of any sentence]
3	-2	Fix for HADOOP-44.  The error string for remote exceptions now contains the full remote stack trace.  Remote exceptions are now also re-thrown on the client as RemoteException rather than IOException, so that they can be distinguished from other IOExceptions.   	Fix[3]for HADOOP -44 .[sentence: 3,-1] The error[-2]string for remote exceptions now contains the full remote stack trace .[sentence: 1,-2] Remote exceptions are now also re -thrown on the client as RemoteException rather than IOException ,so that they can be distinguished from other IOExceptions .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Start namenode before datanodes to minimize datanode startup errors.    	Start namenode before datanodes to minimize datanode startup errors[-2].[sentence: 1,-2]  [result: max + and - of any sentence]
3	-2	Fix for HADOOP-97.  Improve error handling.  Contributed by Konstantin Shvachko.    	Fix[3]for HADOOP -97 .[sentence: 3,-1] Improve[2]error[-2]handling .[sentence: 2,-2] Contributed[2]by Konstantin Shvachko .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-87.  Dont' pass large buffers through to deflater as this is inefficient.    	Fix[3]for HADOOP -87 .[sentence: 3,-1] Dont 'pass large buffers through to deflater as this is inefficient .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-93.  Convert min split size from int to long, and permit its specification in the config.   	Fix[3]for HADOOP -93 .[sentence: 3,-1] Convert min split size from int to long ,and permit its specification in the config .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-2	Fix for HADOOP-86.  Errors while reading map output now cause map task to fail and be re-executed.   	Fix[3]for HADOOP -86 .[sentence: 3,-1] Errors[-2]while reading map output now cause map task to fail and be re -executed .[sentence: 1,-2]  [result: max + and - of any sentence]
3	-1	Give server implementations access to a server's context.  This consists of two additions.  First is a static method Server.get() which returns the server instance it is called under, if any.  Second is the new public class RPC.Server, that replaces a former anonymous class.  RPC server implementation methods can now subclass RPC.Server to keep server state in the subclass.  Application code can then call Server.get() to access that state.  Note that Server.get() may be called under parameter deserialization and return value serialization methods as well, called before and after actual server method calls, respectively.   	Give server implementations access to a server's context .[sentence: 1,-1] This consists of two additions .[sentence: 1,-1] First is a static method Server .[sentence: 1,-1] get ()which returns the server instance it is called under ,if any .[sentence: 1,-1] Second is the new public class RPC .[sentence: 1,-1] Server ,that replaces a former anonymous class .[sentence: 1,-1] RPC server implementation methods can now subclass RPC .[sentence: 1,-1] Server to keep server state in the subclass .[sentence: 1,-1] Application code can then call Server .[sentence: 1,-1] get ()to access that state .[sentence: 1,-1] Note that Server .[sentence: 1,-1] get ()may be called under parameter deserialization and return value serialization methods as well ,called before and after actual server method calls ,respectively[3].[sentence: 3,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-82.  Completes count should never be less than zero.  Contributed by Michael Stack.   	Fix[3]for HADOOP -82 .[sentence: 3,-1] Completes count should never be less than zero .[sentence: 1,-1] Contributed[2]by Michael Stack .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-81.  Job-specific parameters should be read from the job-specific configuration, not the daemon's.  This permits speculative execution, number of map & reduce tasks, etc. to be settable in the job.  Contributed by Owen O'Malley.   	Fix[3]for HADOOP -81 .[sentence: 3,-1] Job -specific parameters should be read from the job -specific configuration ,not the daemon's .[sentence: 1,-1] This permits speculative execution ,number of map &reduce tasks ,etc .[sentence: 1,-1] to be settable in the job .[sentence: 1,-1] Contributed[2]by Owen O'Malley .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-2	Fix for HADOOP-66.  Delete dfs temp files on JVM exit.   	Fix[3]for HADOOP -66 .[sentence: 3,-1] Delete dfs temp files on JVM exit[-2].[sentence: 1,-2]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-80.  Make BytesWritable also a WritableComparable.  Also add hashBytes() utility method to WritableComparator and use it to hash both BytesWritable and UTF8.  Contributed by Owen O'Malley.   	Fix[3]for HADOOP -80 .[sentence: 3,-1] Make BytesWritable also a WritableComparable .[sentence: 1,-1] Also add hashBytes ()utility method to WritableComparator and use it to hash both BytesWritable and UTF8 .[sentence: 1,-1] Contributed[2]by Owen O'Malley .[sentence: 2,-1]  [result: max + and - of any sentence]
1	-1	Updated link to jira.   	Updated link to jira .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-79.  Some namenode optimizations.  Contributed by Konstantin Shvachko.   	Fix[3]for HADOOP -79 .[sentence: 3,-1] Some namenode optimizations .[sentence: 1,-1] Contributed[2]by Konstantin Shvachko .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-2	Fix tasktracker to exit when errors are encountered reading map output, in order to force re-execution of map tasks.  It's overkill, since it will re-compute all map output computed by that task tracker, not just that which could not be read, but this should be a rare situation.  If we start seeing it frequently, then we could optimize this by adding a way to tell the jobtracker that a particular previously completed map task now needs to be re-executed.   	Fix[3]tasktracker to exit[-2]when errors[-2]are encountered reading map output ,in order to force re -execution of map tasks .[sentence: 3,-2] It's overkill ,since it will re -compute all map output computed by that task tracker ,not just that which could not be read ,but this should be a rare situation .[sentence: 1,-1] If we start seeing it frequently ,then we could optimize this by adding a way to tell the jobtracker that a particular previously completed map task now needs to be re -executed .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-2	Fix for HADOOP-78.  Stream buffering was mistakenly disabled in writes by the RPC client.  Identified & fixed by Owen O'Malley.    	Fix[3]for HADOOP -78 .[sentence: 3,-1] Stream buffering was mistakenly[-2]disabled in writes by the RPC client .[sentence: 1,-2] Identified &fixed[3]by Owen O'Malley .[sentence: 3,-1]  [result: max + and - of any sentence]
1	-1	Use build/test for unit test data, instead of /tmp as in defaults.    	Use build /test for unit test data ,instead of /tmp as in defaults .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-77.  Fixes some NPEs.  Contributed by Stefan.   	Fix[3]for HADOOP -77 .[sentence: 3,-1] Fixes[3]some NPEs .[sentence: 3,-1] Contributed[2]by Stefan .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-70.  Unit tests should have their own hadoop-site.xml and mapred-default.xml, so that local modifications to these files in conf/ don't alter unit testing.  Also rename TestDFS so that it is not normally run, and add a new test target which runs tests using the config files in conf/.   	Fix[3]for HADOOP -70 .[sentence: 3,-1] Unit tests should have their own hadoop -site .[sentence: 1,-1] xml and mapred -default .[sentence: 1,-1] xml ,so that local modifications to these files in conf /don't alter unit testing .[sentence: 1,-1] Also rename TestDFS so that it is not normally run ,and add a new test target which runs tests using the config files in conf /.[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Reverted changes from 384385, which removed local backup copy of block & removed most timeouts.  That worked well when all hosts are healthy, but when a few are very slow it caused too many tasks to timeout and loads to balloon on slow hosts.  So the local backup is back, but no longer in /tmp, rather in dfs.data.dir, and timeouts are back.  I also added connect timeouts, so that dfs connects also don't get hung up by slow hosts.   	Reverted changes from 384385 ,which removed local backup copy of block &removed most timeouts .[sentence: 1,-1] That worked well when all hosts are healthy ,but when a few are very slow it caused too many tasks to timeout and loads to balloon on slow hosts .[sentence: 1,-1] So the local backup is back ,but no longer in /tmp ,rather in dfs .[sentence: 1,-1] data .[sentence: 1,-1] dir ,and timeouts are back .[sentence: 1,-1] I also added connect timeouts ,so that dfs connects also don't get hung up by slow hosts .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fixed wiki URL.   	Fixed[3]wiki URL .[sentence: 3,-1]  [result: max + and - of any sentence]
3	-1	Fixed wiki URL.   	Fixed[3]wiki URL .[sentence: 3,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-66.  DFS blocks are no longer written to local temp files.  If a connection to a datanode fails then an exception is now thrown, rather than trying to re-connect to another datanode.  Timeouts were also removed from datanode connections, since these caused a lot of failed connections.   	Fix[3]for HADOOP -66 .[sentence: 3,-1] DFS blocks are no longer written to local temp files .[sentence: 1,-1] If a connection to a datanode fails then an exception is now thrown ,rather than trying to re -connect to another datanode .[sentence: 1,-1] Timeouts were also removed from datanode connections ,since these caused a lot of failed connections .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Add ability to sleep a bit between each command that's submitted to a slave, to meter slave commands a bit.   	Add ability to sleep a bit between each command that's submitted to a slave ,to meter slave commands a bit .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-57.  Permit listing of "/" in dfs.  With help from Mahadev konar.    	Fix[3]for HADOOP -57 .[sentence: 3,-1] Permit listing of '/'in dfs .[sentence: 1,-1] With help from Mahadev konar .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Apply local Configuration to parameters received via RPC if they implement Configurable. Fix by Marko Bauhardt.  Note: in the future there may be protocols that assume different local and remote Configuration. However, with the current RPC implementation we don't support it anyway, and for now it seems better that parameters implementing Configurable should be provided with non-null Configuration.    	Apply local Configuration to parameters received via RPC if they implement Configurable .[sentence: 1,-1] Fix[3]by Marko Bauhardt .[sentence: 3,-1] Note :in the future there may be protocols that assume different local and remote Configuration .[sentence: 1,-1] However ,with the current RPC implementation we don't support it anyway ,and for now it seems better that parameters implementing Configurable should be provided with non -null Configuration .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Increase deflater & inflater buffer size, for better performance.    	Increase deflater &inflater buffer size ,for better performance .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	Reduce iteration through all map & reduce tasks to improve jobtracker performance.    	Reduce iteration through all map &reduce tasks to improve[2]jobtracker performance .[sentence: 2,-1]  [result: max + and - of any sentence]
1	-1	Replace an NPE with an informative warning.    	Replace an NPE with an informative warning .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Increase some intervals to further reduce stress on the jobtracker.    	Increase some intervals to further reduce stress[-2]on the jobtracker .[sentence: 1,-2]  [result: max + and - of any sentence]
1	-1	Don't always query jobtracker for all needed map outputs, instead just for a random sample.  When the total number of splits was large, the jobtracker was spending most of its time servicing these requests. Also reduce the frequency of these requests.  Long-term we may need a different algorithm here to ensure that reduces are more promptly and efficiently notified of map completions.    	Don't always query jobtracker for all needed map outputs ,instead just for a random sample .[sentence: 1,-1] When the total number of splits was large ,the jobtracker was spending most of its time servicing these requests .[sentence: 1,-1] Also reduce the frequency of these requests .[sentence: 1,-1] Long -term we may need a different algorithm here to ensure that reduces are more promptly and efficiently notified of map completions .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	  The MapredLoadTest now has an extra step, to exercise the case where we have multiple reduce tasks.    It used to have two stages: one job that created a huge file of numbers in random order, followed by a job that would read that file and count the numbers.  If the final count was correct, the test passed.    Unfortunately, neither of these jobs had a reduce task that was greater than 1.    So now we've got three stages.  The first stage is unchanged.  The second stage reads the big file, then emits the answer key split into 10 parts, one for each reduce task.  then a third stage merges those parts into a final number count.  As before, if that final count is correct, all is well.     	The MapredLoadTest now has an extra step ,to exercise the case where we have multiple reduce tasks .[sentence: 1,-1] It used to have two stages :one job that created a huge file of numbers in random order ,followed by a job that would read that file and count the numbers .[sentence: 1,-1] If the final count was correct ,the test passed .[sentence: 1,-1] Unfortunately[-2],neither of these jobs had a reduce task that was greater than 1 .[sentence: 1,-2] So now we've got three stages .[sentence: 1,-1] The first stage is unchanged .[sentence: 1,-1] The second stage reads the big file ,then emits the answer key split into 10 parts ,one for each reduce task .[sentence: 1,-1] then a third stage merges those parts into a final number count .[sentence: 1,-1] As before ,if that final count is correct ,all is well .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	  Fix bug HADOOP-26.  Available space is now considered correctly during DFS block-allocation.      	Fix[3]bug HADOOP -26 .[sentence: 3,-1] Available space is now considered correctly during DFS block -allocation .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	  This code makes sure we read from a local block, if available.  I thought this code had already been committed some time ago, but the workspace doesn't have it.     	This code makes sure we read from a local block ,if available .[sentence: 1,-1] I thought this code had already been committed some time ago ,but the workspace doesn't have it .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fix a couple of tasktracker bugs.   	Fix[3]a couple of tasktracker bugs .[sentence: 3,-1]  [result: max + and - of any sentence]
3	-1	  A bug in the TaskTracker was governing task-allocation by counting the total number of tasks.  The right thing to do is keep a total for map tasks, and a separate total for reduces.    This is now fixed.     	A bug in the TaskTracker was governing task -allocation by counting the total number of tasks .[sentence: 1,-1] The right thing to do is keep a total for map tasks ,and a separate total for reduces .[sentence: 1,-1] This is now fixed[3].[sentence: 3,-1]  [result: max + and - of any sentence]
1	-1	Check taskid's more carefully.  Suggested by Michael Stack.    	Check taskid's more carefully .[sentence: 1,-1] Suggested by Michael Stack .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-16.  Splitting and other job planning is now performed in a separate thread.   	Fix[3]for HADOOP -16 .[sentence: 3,-1] Splitting and other job planning is now performed in a separate thread .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Upgrade to Lucene 1.9.1.   	Upgrade to Lucene 1 .[sentence: 1,-1] 9 .[sentence: 1,-1] 1 .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-60, with help from Owen & Michael.   	Fix[3]for HADOOP -60 ,with help from Owen &Michael .[sentence: 3,-1]  [result: max + and - of any sentence]
1	-1	Permit folks to modify options passed to ssh.  For example, older versions of ssh do not support the ConnectTimeout option.   	Permit folks to modify options passed to ssh .[sentence: 1,-1] For example ,older versions of ssh do not support the ConnectTimeout option .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Add webapps to classpath so they're found.    	Add webapps to classpath so they're found .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fix rsync command to (a) use ssh as transport, and (b) to correctly quote the path.    	Fix[3]rsync command to (a )use ssh as transport ,and (b )to correctly quote the path .[sentence: 3,-1]  [result: max + and - of any sentence]
3	-1	Fix to make independent of "unzip", using java's built in unzip code instead.    	Fix[3]to make independent of 'unzip ',using java's built in unzip code instead .[sentence: 3,-1]  [result: max + and - of any sentence]
3	-1	Fix command line (again!).    	Fix[3]command line (again !).[+0.6 punctuation mood emphasis][sentence: 3,-1]  [result: max + and - of any sentence]
1	-1	Update example command line.    	Update example command line .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Un-escape containing jar's path, which is URL-encoded.  This fixes things primarily on Windows, where paths are likely to contain spaces.    	Un -escape containing jar's path ,which is URL -encoded .[sentence: 1,-1] This fixes[3]things primarily on Windows ,where paths are likely to contain spaces .[sentence: 3,-1]  [result: max + and - of any sentence]
1	-1	Escape paths so that spaces are permitted (as is common on Windows.)   	Escape paths so that spaces are permitted (as is common on Windows .)[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Upgrade lucene version to final release.   	Upgrade lucene version to final release .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	Minor improvements to DOAP file.    	Minor improvements[2]to DOAP file .[sentence: 2,-1]  [result: max + and - of any sentence]
1	-1	Add a DOAP description for hadoop.   	Add a DOAP description for hadoop .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Print stack trace when child fails to contact parent.    	Print stack trace when child fails to contact parent .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-40.  Buffer size was ignored.   	Fix[3]for HADOOP -40 .[sentence: 3,-1] Buffer size was ignored .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-41.  Support passing more options to child JVM.  Contributed by Michael Stack.   	Fix[3]for HADOOP -41 .[sentence: 3,-1] Support passing more options to child JVM .[sentence: 1,-1] Contributed[2]by Michael Stack .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-40.  Buffer position was not maintained correctly.  Contributed by Konstantin Shvachko.   	Fix[3]for HADOOP -40 .[sentence: 3,-1] Buffer position was not maintained correctly .[sentence: 1,-1] Contributed[2]by Konstantin Shvachko .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-49: Permit specification of jobtracker when submitting jobs.   	Fix[3]for HADOOP -49 :Permit specification of jobtracker when submitting jobs .[sentence: 3,-1]  [result: max + and - of any sentence]
3	-2	Fix so that task state is displayed even when there are no errors.  Also changed report to be a datastructure rather than a vector of strings.   	Fix[3]so that task state is displayed even when there are no errors[-2].[sentence: 3,-2] Also changed report to be a datastructure rather than a vector of strings .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Updated javadoc for recent config changes.   	Updated javadoc for recent config changes .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Add missing synchronization.   	Add missing synchronization .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Make speculative execution optional, since it can break map tasks that have side effects.  Also fix TestFileSystem to be safe to use with speculative execution.   	Make speculative execution optional ,since it can break map tasks that have side effects .[sentence: 1,-1] Also fix[3]TestFileSystem to be safe to use with speculative execution .[sentence: 3,-1]  [result: max + and - of any sentence]
3	-2	Fix HADOOP-36.  Scripts now source conf/hadoop-env.sh, to faciliate setting of environment variables, even on remote hosts.  The default slaves file has move from ~/.slaves to conf/slaves.   	Fix[3]HADOOP -36 .[sentence: 3,-1] Scripts now source[-2]conf /hadoop -env .[sentence: 1,-2] sh ,to faciliate setting of environment variables ,even on remote hosts .[sentence: 1,-1] The default slaves file has move from ~/.[sentence: 1,-1] slaves to conf /slaves .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fix HADOOP-38: Add FileSystem.getBlockSize() method and use it as the maximum split size.  Also change FileSystem to implement Configurable, and improve some javadoc, using inherited comments where possible and removing implementation details from public javadoc.   	Fix[3]HADOOP -38 :Add FileSystem .[sentence: 3,-1] getBlockSize ()method and use it as the maximum split size .[sentence: 1,-1] Also change FileSystem to implement Configurable ,and improve[2]some javadoc ,using inherited comments where possible and removing implementation details from public javadoc .[sentence: 2,-1]  [result: max + and - of any sentence]
1	-1	Move Closeable interface to io package, since it is of general utility, and to prepare for JDK 1.5.   	Move Closeable interface to io package ,since it is of general utility ,and to prepare for JDK 1 .[sentence: 1,-1] 5 .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-2	  Fix bug HADOOP-16.    Don't invoke TaskInProgress.hasTaskWithCacheHit() unnecessarily from within JobInProgress.    Also, cache filesystem hints inside JobInProgress.     	Fix[3]bug HADOOP -16 .[sentence: 3,-1] Don't invoke TaskInProgress .[sentence: 1,-1] hasTaskWithCacheHit ()unnecessarily[-2]from within JobInProgress .[sentence: 1,-2] Also ,cache filesystem hints inside JobInProgress .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	HADOOP-37: Add ClusterStatus.  Contributed by Owen O'Malley.   	HADOOP -37 :Add ClusterStatus .[sentence: 1,-1] Contributed[2]by Owen O'Malley .[sentence: 2,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-34: make build paths relative to location of build.xml, not PWD.  Contributed by Jeremy Bensley.   	Fix[3]for HADOOP -34 :make build paths relative to location of build .[sentence: 3,-1] xml ,not PWD .[sentence: 1,-1] Contributed[2]by Jeremy Bensley .[sentence: 2,-1]  [result: max + and - of any sentence]
1	-1	  Add a bunch of updated comments and JavaDocs to the Distributed File System package.     	Add a bunch of updated comments and JavaDocs to the Distributed File System package .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Bundle webapps into jar.   	Bundle webapps into jar .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fix HADOOP-30: add -lsr and -cat commands to DFSShell.  Contributed by Michel Tourn.   	Fix[3]HADOOP -30 :add -lsr and -cat commands to DFSShell .[sentence: 3,-1] Contributed[2]by Michel Tourn .[sentence: 2,-1]  [result: max + and - of any sentence]
2	-1	Further improvements from Bryan A. Pendleton.    	Further improvements[2]from Bryan A .[sentence: 2,-1] Pendleton .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Make compatible with JDK 1.4.    	Make compatible with JDK 1 .[sentence: 1,-1] 4 .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-12.  The JobTracker now loads the InputFormat from the job's jar file.   	Fix[3]for HADOOP -12 .[sentence: 3,-1] The JobTracker now loads the InputFormat from the job's jar file .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Keep 'unzip' from prompting when overwriting (e.g., when archive contains same file twice).  Also make it less verbose.    	Keep 'unzip 'from prompting when overwriting (e .[sentence: 1,-1] g .,[sentence: 1,-1] when archive contains same file twice ).[sentence: 1,-1] Also make it less verbose .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Add an easy way to specify a job's jar, by naming a class in the jar.   	Add an easy way to specify a job's jar ,by naming a class in the jar .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Stop using deprecated 'jspc' ant task and instead call jasper directly.  Also rename webapp files to align with package name.   	Stop using deprecated 'jspc 'ant task and instead call jasper directly .[sentence: 1,-1] Also rename webapp files to align with package name .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fixed HADOOP-20: permit mappers and reducers to cleanup.  Add a close() method to the Mapper and Reducer interfaces by having them extend a Closeable interface.  Update all implementations to define close().  Patch by Michel Tourn.   	Fixed[3]HADOOP -20 :permit mappers and reducers to cleanup .[sentence: 3,-1] Add a close ()method to the Mapper and Reducer interfaces by having them extend a Closeable interface .[sentence: 1,-1] Update all implementations to define close ().[sentence: 1,-1] Patch by Michel Tourn .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Removed properties mistakenly copied from Nutch.    	Removed properties mistakenly[-2]copied from Nutch .[sentence: 1,-2]  [result: max + and - of any sentence]
1	-1	Create html documentation for configuration defaults and link to these from javadoc.   	Create html documentation for configuration defaults and link to these from javadoc .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fix for split from nutch.    	Fix[3]for split from nutch .[sentence: 3,-1]  [result: max + and - of any sentence]
3	-1	Fixed so that examples jar is packaged correctly.    	Fixed[3]so that examples jar is packaged correctly .[sentence: 3,-1]  [result: max + and - of any sentence]
3	-1	Fix HADOOP-28.  Jsp pages are now pre-compiled to servlets that can access package-private classes.   	Fix[3]HADOOP -28 .[sentence: 3,-1] Jsp pages are now pre -compiled to servlets that can access package -private classes .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fix HADOOP-25: improve example code & package separately.  Contributed by Owen O'Malley.   	Fix[3]HADOOP -25 :improve[2]example code &package separately .[sentence: 3,-1] Contributed[2]by Owen O'Malley .[sentence: 2,-1]  [result: max + and - of any sentence]
1	-1	Updated javadoc.    	Updated javadoc .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Initial version of a Hadoop tutorial.    	Initial version of a Hadoop tutorial .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Clean up javadoc, reduce number of public classes.   	Clean up javadoc ,reduce number of public classes .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Log to logs directory by default.    	Log to logs directory by default .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Remove a spurious space.    	Remove a spurious space .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Add WritableFactory mechanism, to permit one to use ObjectWritable with non-public classes.  Register factories for DFS implementation classes and make them non-public.  This greatly simplifies the javadoc.   	Add WritableFactory mechanism ,to permit one to use ObjectWritable with non -public classes .[sentence: 1,-1] Register factories for DFS implementation classes and make them non -public .[sentence: 1,-1] This greatly simplifies the javadoc .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	Improved javadoc, starting overview and package documentation.  Also moved DF from dfs to fs package.   	Improved[2]javadoc ,starting overview and package documentation .[sentence: 2,-1] Also moved DF from dfs to fs package .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Add scripts into jar file so they're bundled with code.   	Add scripts into jar file so they're bundled with code .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fixed javadoc link and added more news.   	Fixed[3]javadoc link and added more news .[sentence: 3,-1]  [result: max + and - of any sentence]
3	-1	Fix for HADOOP-22: remove unused imports.  By Sami Siren.   	Fix[3]for HADOOP -22 :remove unused imports .[sentence: 3,-1] By Sami Siren .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fix HADOOP-21: fix mapred webapp jsp for split from Nutch.  Contributed by Owen O'Malley.   	Fix[3]HADOOP -21 :fix[3]mapred webapp jsp for split from Nutch .[sentence: 3,-1] Contributed[2]by Owen O'Malley .[sentence: 2,-1]  [result: max + and - of any sentence]
1	-1	Remove vestige of Nutch's build.xml so that nightly target will run.    	Remove vestige of Nutch's build .[sentence: 1,-1] xml so that nightly target will run .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Fix HADOOP-5: add commons logging jar to lib.   	Fix[3]HADOOP -5 :add commons logging jar to lib .[sentence: 3,-1]  [result: max + and - of any sentence]
3	-1	Remove a few vestiges of Nutch's plugin mechanism, fixing bug HADOOP-6 (Thanks, Owen!).  Also fix a few typos.    	Remove a few vestiges of Nutch's plugin mechanism ,fixing[3]bug HADOOP -6 (Thanks[2],Owen !).[+0.6 punctuation mood emphasis][sentence: 3,-1] Also fix[3]a few typos .[sentence: 3,-1]  [result: max + and - of any sentence]
3	-2	Fix some config problems, remove notion of app resources: it's overkill.   	Fix[3]some config problems[-2],remove notion of app resources :it's overkill .[sentence: 3,-2]  [result: max + and - of any sentence]
1	-1	Don't require final resoureces.   	Don't require final resoureces .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Permit multiple default and final Configuration resources.   	Permit multiple default and final Configuration resources .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	First version that passes unit tests.   	First version that passes unit tests .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	First version that compiles.   	First version that compiles .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Initial commit of code copied from Nutch.   	Initial commit of code copied from Nutch .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Initial commit of code copied from Nutch.   	Initial commit of code copied from Nutch .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Slightly improved logo.   	Slightly improved[2][+-1 booster word]logo .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	First version of website.   	First version of website .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Create hadoop sub-project.   	Create hadoop sub -project .[sentence: 1,-1]  [result: max + and - of any sentence]
