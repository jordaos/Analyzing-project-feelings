Positive	Negative	Text	Explanation
1	-1	The old core branches are now common branches.    	The old core branches are now common branches .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Moving branches to new location.    	Moving branches to new location .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Merged fixes from trunk, preparing for 0.2.1 release.   	Merged fixes from trunk ,preparing for 0 .[sentence: 1,-1] 2 .[sentence: 1,-1] 1 release .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Preparing to tag 0.1.0 release.   	Preparing to tag 0 .[sentence: 1,-1] 1 .[sentence: 1,-1] 0 release .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Adding branch for 0.2 releases.  2nd attempt.   	Adding branch for 0 .[sentence: 1,-1] 2 releases .[sentence: 1,-1] 2nd attempt .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Quote HADOOP_HOME so that it may contain spaces.    	Quote HADOOP _HOME so that it may contain spaces .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	HADOOP-96.  Logging improvements.  Contributed by Hairong Kuang.   	HADOOP -96 .[sentence: 1,-1] Logging improvements[2].[sentence: 2,-1] Contributed by Hairong Kuang .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	HADOOP-198.  Add more example programs to example driver.  Also fix another WritableFactories-related error.  Contributed by Mahadev.   	HADOOP -198 .[sentence: 1,-1] Add more example programs to example driver .[sentence: 1,-1] Also fix another WritableFactories -related error[-2].[sentence: 1,-2] Contributed by Mahadev .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	HADOOP-182.  Fix problems related to lost task trackers.   	HADOOP -182 .[sentence: 1,-1] Fix problems[-2]related to lost task trackers .[sentence: 1,-2]  [result: max + and - of any sentence]
1	-1	HADOOP-193 & HADOOP-194.  A filesystem benchmark and a filesystem checker.  Contributed by Konstantin.   	HADOOP -193 &HADOOP -194 .[sentence: 1,-1] A filesystem benchmark and a filesystem checker .[sentence: 1,-1] Contributed by Konstantin .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	HADOOP-65.  Initial version of multi-language record system.  Contributed by Milind Bhandarkar.   	HADOOP -65 .[sentence: 1,-1] Initial version of multi -language record system .[sentence: 1,-1] Contributed by Milind Bhandarkar .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Fix a problem introduced by the patch for HADOOP-184, where the 'package' target was broken.  Contributed by Mahadev.   	Fix a problem[-2]introduced by the patch for HADOOP -184 ,where the 'package 'target was broken[-2].[sentence: 1,-2] Contributed by Mahadev .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Better fix for HADOOP-189.  Tested in both distributed and standalone mode.  Note that WritableFactories is not quite functioning correctly and there are a few hacks around this that should be removed when that's better understood.   	Better fix for HADOOP -189 .[sentence: 1,-1] Tested in both distributed and standalone mode .[sentence: 1,-1] Note that WritableFactories is not quite functioning correctly and there are a few hacks around this that should be removed when that's better understood .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Reverting change 399426, which broke distributed operation.   	Reverting[-2]change 399426 ,which broke[-2]distributed operation .[sentence: 1,-2]  [result: max + and - of any sentence]
1	-1	HADOOP-191.  Add streaming contrib package.  Contributed by Michel Tourn.   	HADOOP -191 .[sentence: 1,-1] Add streaming contrib package .[sentence: 1,-1] Contributed by Michel Tourn .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-3	HADOOP-192.  Fix a Java 1.4 incompatibility.  Contributed by David Bowen.   	HADOOP -192 .[sentence: 1,-1] Fix a Java 1 .[sentence: 1,-1] 4 incompatibility[-3].[sentence: 1,-3] Contributed by David Bowen .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	HADOOP-189.  Fix MapReduce in standalone configuration to correctly handle job jar files that contain a lib directory with nested jar files.   	HADOOP -189 .[sentence: 1,-1] Fix MapReduce in standalone configuration to correctly handle job jar files that contain a lib directory with nested jar files .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	HADOOP-184.  Restructure some test code to better support testing on a cluster.  Contributed by Mahadev.   	HADOOP -184 .[sentence: 1,-1] Restructure some test code to better support[2]testing on a cluster .[sentence: 2,-1] Contributed by Mahadev .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix HADOOP-19.  If a child process hangs after it has reported completion its output should not be lost.   	Fix HADOOP -19 .[sentence: 1,-1] If a child process hangs after it has reported completion its output should not be lost .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	HADOOP-185.  Fix so that, if a task tracker times out making the RPC asking for a new task to run, the job tracker does not think that it is actually running the task returned (but never received).  Contributed by Owen.   	HADOOP -185 .[sentence: 1,-1] Fix so that ,if a task tracker times out making the RPC asking for a new task to run ,the job tracker does not think that it is actually running the task returned (but never received ).[sentence: 1,-1] Contributed by Owen .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	HADOOP-188.  More fixes to to JobClient, following on HADOOP-174.  Contributed by Owen.   	HADOOP -188 .[sentence: 1,-1] More fixes to to JobClient ,following on HADOOP -174 .[sentence: 1,-1] Contributed by Owen .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	HADOOP-187.  Add RandomWriter and Sort examples.  Contributed by Owen.   	HADOOP -187 .[sentence: 1,-1] Add RandomWriter and Sort examples .[sentence: 1,-1] Contributed by Owen .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-2	HADOOP-186.  Better error handling in TaskTracker's top-level loop.  Also improve calculation of time to send next heartbeat.  Contributed by Owen O'Malley.   	HADOOP -186 .[sentence: 1,-1] Better error[-2]handling in TaskTracker's top -level loop .[sentence: 1,-2] Also improve[2]calculation of time to send next heartbeat .[sentence: 2,-1] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	HADOOP-183.  If the namendode is restarted with different minimum or maximum replication counts, existing files' replication counts are now automatically adjusted to be within the newly configured bounds.  Contributed by Hairong Kuang.   	HADOOP -183 .[sentence: 1,-1] If the namendode is restarted with different minimum or maximum replication counts ,existing files 'replication counts are now automatically adjusted to be within the newly configured bounds .[sentence: 1,-1] Contributed by Hairong Kuang .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	HADOOP-178.  Piggyback DFS blockwork requests on heartbeat responses, reducing traffic.  Also move blockwork delay on startup from datanode to namenode, fixing a problems when the namenode alone restarts.  Contributed by Hairong Kuang.   	HADOOP -178 .[sentence: 1,-1] Piggyback DFS blockwork[-2]requests on heartbeat responses ,reducing traffic .[sentence: 1,-2] Also move blockwork[-2]delay[-2]on startup from datanode to namenode ,fixing a problems[-2]when the namenode alone restarts .[sentence: 1,-2] Contributed by Hairong Kuang .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	HADOOP-177.  Page through tasks in web ui.   	HADOOP -177 .[sentence: 1,-1] Page through tasks in web ui .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix HADOOP-174.  Make job client try up to five times to contact job tracker before aborting a job.  Contributed by Owen.   	Fix HADOOP -174 .[sentence: 1,-1] Make job client try up to five times to contact job tracker before aborting a job .[sentence: 1,-1] Contributed by Owen .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Note recent changes.   	Note recent changes .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-2	Fix createNewFile() so that it creates a .crc file, like everything else does.  This fixes some annoyances in Nutch.    	Fix createNewFile ()so that it creates a .[sentence: 1,-1] crc file ,like[2]everything else does .[sentence: 2,-1] This fixes some annoyances[-3][--1 booster word]in Nutch .[sentence: 1,-2]  [result: max + and - of any sentence]
1	-1	Fix HADOOP-167.  Reduce the number of Configuration and JobConf's allocated.  Contributed by Owen.   	Fix HADOOP -167 .[sentence: 1,-1] Reduce the number of Configuration and JobConf's allocated .[sentence: 1,-1] Contributed by Owen .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Change default replication.max in code to be the same as in hadoop-default.xml.    	Change default[-2]replication .[sentence: 1,-2] max in code to be the same as in hadoop -default[-2].[sentence: 1,-2] xml .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	HADOOP-173.  Optimize allocation of tasks with local data.   	HADOOP -173 .[sentence: 1,-1] Optimize allocation of tasks with local data .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-2	Permit configuration to specify higher replication for job submission files.  Also reduce complaints when a file's replication is greater than the size of the cluster.   	Permit configuration to specify higher replication for job submission files .[sentence: 1,-1] Also reduce complaints[-2]when a file's replication is greater[3]than the size of the cluster .[sentence: 3,-2]  [result: max + and - of any sentence]
2	-2	Fix bug introduced yesterday.  NullInstance really is still required!   	Fix bug[-2]introduced yesterday .[sentence: 1,-2] NullInstance really is still required ![+0.6 punctuation mood emphasis][sentence: 2,-1]  [result: max + and - of any sentence]
1	-1	Fix HADOOP-170.  Permit FileSystem clients to examine and modify the replication count of individual files.  Also fix a few replication-related bugs.  Contributed by Konstantin Shvachko.   	Fix HADOOP -170 .[sentence: 1,-1] Permit FileSystem clients to examine and modify the replication count of individual files .[sentence: 1,-1] Also fix a few replication -related bugs .[sentence: 1,-1] Contributed by Konstantin Shvachko .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-3	Fix HADOOP-169.  Don't fail reduce tasks if a call to the jobtracker to locate map outputs fails.  Contributed by Owen.   	Fix HADOOP -169 .[sentence: 1,-1] Don't fail[-3][=0 negation]reduce tasks if a call to the jobtracker to locate map outputs fails[-3].[sentence: 1,-3] Contributed by Owen .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix HADOOP-168.  Add IOException to throws of all MapReduce RPC protocol methods.  Contributed by Owen.   	Fix HADOOP -168 .[sentence: 1,-1] Add IOException to throws of all MapReduce RPC protocol methods .[sentence: 1,-1] Contributed by Owen .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-3	Fix HADOOP-166.  RPCs can now pass subclasses of declared types as parameters.  Note this change is incompatible for any application that stores ObjectWritables in a file.  Nutch only stores ObjectWritable in temporary intermediate files, so this is not a problem for Nutch.   	Fix HADOOP -166 .[sentence: 1,-1] RPCs can now pass subclasses of declared types as parameters .[sentence: 1,-1] Note this change is incompatible[-3]for any application that stores ObjectWritables in a file .[sentence: 1,-3] Nutch only stores ObjectWritable in temporary intermediate files ,so this is not a problem[-2]for Nutch .[sentence: 1,-2]  [result: max + and - of any sentence]
1	-1	Remove some Java 1.5 dependencies from the new metrics code.   	Remove some Java 1 .[sentence: 1,-1] 5 dependencies from the new metrics code .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix HADOOP-160.  Remove some uneeded synchronization around time-consuming operations in the TaskTracker.   	Fix HADOOP -160 .[sentence: 1,-1] Remove some uneeded synchronization around time -consuming operations in the TaskTracker .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Add attribution.    	Add attribution .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix HADOOP-132.  Add an API for reporting metrics (as yet unused).  Contributed by David Bowen.   	Fix HADOOP -132 .[sentence: 1,-1] Add an API for reporting metrics (as yet unused ).[sentence: 1,-1] Contributed by David Bowen .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix HADOOP-162.  Stop generating ConcurrentModificationExceptions when releasing file locks.  Contributed by Owen O'Malley.   	Fix HADOOP -162 .[sentence: 1,-1] Stop generating ConcurrentModificationExceptions when releasing file locks .[sentence: 1,-1] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-154 (fixed in other relevant places, too).    	Fix for HADOOP -154 (fixed in other relevant places ,too ).[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	Fix HADOOP-150.  Improved task names that include job name.   	Fix HADOOP -150 .[sentence: 1,-1] Improved[2]task names that include job name .[sentence: 2,-1]  [result: max + and - of any sentence]
1	-3	Fix HADOOP-157.  Make dfs client wait long enough for locks on abandoned files to expire when creating files, so that when a task that writes to dfs fails, its replacements do not also immediately fail when they try to open the same files.  Contributed by Owen O'Malley.   	Fix HADOOP -157 .[sentence: 1,-1] Make dfs client wait long enough for locks on abandoned[-2]files to expire when creating files ,so that when a task that writes to dfs fails[-3],its replacements do not also immediately fail[-3]when they try to open the same files .[sentence: 1,-3] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix HADOOP-69.  NPE when getting hints for a non-existant file chunk.  Contributed by Bryan Pendelton.   	Fix HADOOP -69 .[sentence: 1,-1] NPE when getting hints for a non -existant file chunk .[sentence: 1,-1] Contributed by Bryan Pendelton .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Fix HADOOP-151.  Close a potential socket leak.   	Fix HADOOP -151 .[sentence: 1,-1] Close a potential socket leak[-2].[sentence: 1,-2]  [result: max + and - of any sentence]
1	-3	Fix HADOOP-148.  Maintain a task failure count per tasktracker and display it in the web ui.  Contributed by Owen.   	Fix HADOOP -148 .[sentence: 1,-1] Maintain a task failure[-3]count per tasktracker and display it in the web ui .[sentence: 1,-3] Contributed by Owen .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-3	Fix for HADOOP-142.  Avoid re-running a task on a node where it has previously failed.  Contributed by Owen.   	Fix for HADOOP -142 .[sentence: 1,-1] Avoid[-2]re -running a task on a node where it has previously failed[-3].[sentence: 1,-3] Contributed by Owen .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-3	Fix for HADOOP-133.  Retry pings from child to parent, in case of (local) communcation problems.  Also log exit status, so that one can distinguish patricide from other deaths.  Contributed by Owen.   	Fix for HADOOP -133 .[sentence: 1,-1] Retry pings from child to parent ,in case of (local )communcation problems[-2].[sentence: 1,-2] Also log exit[-2]status ,so that one can distinguish patricide from other deaths[-3].[sentence: 1,-3] Contributed by Owen .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Log the last two commits.    	Log the last two commits .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-134.  Don't hang jobs when the tasktracker is misconfigured to use an un-writable local directory.  Contributed by Owen.   	Fix for HADOOP -134 .[sentence: 1,-1] Don't hang[-2][=0 negation]jobs when the tasktracker is misconfigured to use an un -writable local directory .[sentence: 1,-1] Contributed by Owen .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Fix HADOOP-114.  Error message named wrong config property.  Contributed by Michael Stack.   	Fix HADOOP -114 .[sentence: 1,-1] Error[-2]message named wrong[-2]config property .[sentence: 1,-2] Contributed by Michael Stack .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Fix for HADOOP-139.  Fix a potential deadlock in LocalFileSystem.lock().  Contributed by Igor Bolotin.   	Fix for HADOOP -139 .[sentence: 1,-1] Fix a potential deadlock[-2]in LocalFileSystem .[sentence: 1,-2] lock ().[sentence: 1,-1] Contributed by Igor Bolotin .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix HADOOP-138.  Stop multiple tasks per heartbeat.  Contributed by Stefan.   	Fix HADOOP -138 .[sentence: 1,-1] Stop multiple tasks per heartbeat .[sentence: 1,-1] Contributed by Stefan .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-2	Fix HADOOP-118.  Improved cleanup of abandoned file creations in DFS.  Contributed by Owen.   	Fix HADOOP -118 .[sentence: 1,-1] Improved[2]cleanup of abandoned[-2]file creations in DFS .[sentence: 2,-2] Contributed by Owen .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix HADOOP-143.  Stop line-wrapping when displaying stack traces.  Contributed by Owen O'Malley.   	Fix HADOOP -143 .[sentence: 1,-1] Stop line -wrapping when displaying stack traces .[sentence: 1,-1] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix HADOOP-144.  Use mapred task id as dfs client id to faciliate debugging.  Contributed by Owen O'Malley.   	Fix HADOOP -144 .[sentence: 1,-1] Use mapred task id as dfs client id to faciliate debugging .[sentence: 1,-1] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fixed HADOOP-129.  Replaced uses of java.io.File in FileSystem API with a new class named Path.  Also dfs.local.dir and mapred.local.dir may no longer be space-separated, but must now be comma-separated lists of directories.   	Fixed HADOOP -129 .[sentence: 1,-1] Replaced uses of java .[sentence: 1,-1] io .[sentence: 1,-1] File in FileSystem API with a new class named Path .[sentence: 1,-1] Also dfs .[sentence: 1,-1] local .[sentence: 1,-1] dir and mapred .[sentence: 1,-1] local .[sentence: 1,-1] dir may no longer be space -separated ,but must now be comma -separated lists of directories .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Updated change log with recent changes.    	Updated change log with recent changes .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-2	Fix HADOOP-128.  Improved DFS error handling.  Contributed by Owen O'Malley.   	Fix HADOOP -128 .[sentence: 1,-1] Improved[2]DFS error[-2]handling .[sentence: 2,-2] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-92.  Show information about all attempts to run each task in the web ui.  Contributed by Mahadev konar.   	Fix for HADOOP -92 .[sentence: 1,-1] Show information about all attempts to run each task in the web ui .[sentence: 1,-1] Contributed by Mahadev konar .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Add some logging during shuffle.    	Add some logging during shuffle .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Add link to store.   	Add link to store .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Stop using ssh options by default that are not yet in widely used versions of ssh.  Folks can still enable their use by uncommenting a line in conf/hadoop-env.sh.   	Stop using ssh options by default[-2]that are not yet in widely used versions of ssh .[sentence: 1,-2] Folks can still enable their use by uncommenting a line in conf /hadoop -env .[sentence: 1,-1] sh .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix HADOOP-131.  Add scripts to start/stop dfs and mapred daemons.  Use these in start/stop-all scripts.  Contributed by Chris Mattmann.   	Fix HADOOP -131 .[sentence: 1,-1] Add scripts to start /stop dfs and mapred daemons .[sentence: 1,-1] Use these in start /stop -all scripts .[sentence: 1,-1] Contributed by Chris Mattmann .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	Fix script documentation.  Thanks, Stefan!    	Fix script documentation .[sentence: 1,-1] Thanks[2],Stefan ![+0.6 punctuation mood emphasis][sentence: 2,-1]  [result: max + and - of any sentence]
2	-1	Fix for HADOOP-51.  Support per-file replication counts in DFS.  Contributed by Konstantin Shvachko.   	Fix for HADOOP -51 .[sentence: 1,-1] Support[2]per -file replication counts in DFS .[sentence: 2,-1] Contributed by Konstantin Shvachko .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-126.  'bin/hadoop dfs -cp' now correctly handles .crc files.  This also consolidates a lot of file copying code.  Contributed by Konstantin Shvachko.   	Fix for HADOOP -126 .[sentence: 1,-1] 'bin /hadoop dfs -cp 'now correctly handles .[sentence: 1,-1] crc files .[sentence: 1,-1] This also consolidates a lot of file copying code .[sentence: 1,-1] Contributed by Konstantin Shvachko .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Preparing for 0.1.1 release.   	Preparing for 0 .[sentence: 1,-1] 1 .[sentence: 1,-1] 1 release .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Fix for HADOOP-125.  Absolute paths are tricky on Windows.  For Hadoop's purposes, consider things that start with a slash to be absolute.  Also, Hadoop should not change the JVM's CWD.  All files are now correctly cleaned up for a Nutch crawl, in either local or psuedo-distributed mode.   	Fix for HADOOP -125 .[sentence: 1,-1] Absolute paths are tricky[-2]on Windows .[sentence: 1,-2] For Hadoop's purposes ,consider things that start with a slash[-2]to be absolute .[sentence: 1,-2] Also ,Hadoop should not change the JVM's CWD .[sentence: 1,-1] All files are now correctly cleaned up for a Nutch crawl ,in either local or psuedo -distributed mode .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Remove .crc files too.    	Remove .[sentence: 1,-1] crc files too .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix HADOOP-116.  Clean up job submission files.  On job completion, remove the directory containing the submitted job.xml file, since JobClient always creates a new directory to hold this file.   	Fix HADOOP -116 .[sentence: 1,-1] Clean up job submission files .[sentence: 1,-1] On job completion ,remove the directory containing the submitted job .[sentence: 1,-1] xml file ,since JobClient always creates a new directory to hold this file .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix HADOOP-117.  Correctly remove mapred temp files.   	Fix HADOOP -117 .[sentence: 1,-1] Correctly remove mapred temp files .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Update change log.    	Update change log .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix so that close() throws IOException, so that classes which override this method can throw IOException, as specified in the Closeable interface.  Also add the Apache license (which must be in every file) and add a bit more javadoc.    	Fix so that close ()throws IOException ,so that classes which override this method can throw IOException ,as specified in the Closeable interface .[sentence: 1,-1] Also add the Apache license (which must be in every file )and add a bit more javadoc .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Trunk is now 0.2-dev.    	Trunk is now 0 .[sentence: 1,-1] 2 -dev .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Updating the copyright year.    	Updating the copyright year .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	Starting a Hadoop Change Log.  Developers: please add a note to this file for each significant change.  Contributed patches should ideally include an entry for this file.    	Starting a Hadoop Change Log .[sentence: 1,-1] Developers :please[2]add a note to this file for each significant change .[sentence: 2,-1] Contributed patches should ideally include an entry for this file .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Update site for 0.1.0 release.   	Update site for 0 .[sentence: 1,-1] 1 .[sentence: 1,-1] 0 release .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	More fixes to get working directory to work on Windows.  Long-term we should stop using java.io.File for our abstract file paths.  On Windows, 'new File(/foo/bar).isAbsolute()' returns false, which caused lots of problems.  So I have added a FileSystem.isAbsolute() method that we use internally.  I also added a few more .getAbsoluteFile() calls to convert paths into absolute paths so that LocalFileSystem works correctly.  Finally I took advantage of file status information cached in DFSFile, eliminating some namenode RPCs.   	More fixes to get working directory to work on Windows .[sentence: 1,-1] Long -term we should stop using java .[sentence: 1,-1] io .[sentence: 1,-1] File for our abstract file paths .[sentence: 1,-1] On Windows ,'new File (/foo /bar ).[sentence: 1,-1] isAbsolute ()'returns false ,which caused lots of problems[-2].[sentence: 1,-2] So I have added a FileSystem .[sentence: 1,-1] isAbsolute ()method that we use internally .[sentence: 1,-1] I also added a few more .[sentence: 1,-1] getAbsoluteFile ()calls to convert paths into absolute paths so that LocalFileSystem works correctly .[sentence: 1,-1] Finally I took advantage of file status information cached in DFSFile ,eliminating some namenode RPCs .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix wiki url.    	Fix wiki url .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-3	If close() fails, then abandon the file, so that any leases are cleared and other task may attempt to create it.    	If close ()fails[-3],then abandon[-2]the file ,so that any leases are cleared and other task may attempt to create it .[sentence: 1,-3]  [result: max + and - of any sentence]
1	-2	Fix a bug where writing zero-length files would cause things to hang.    	Fix a bug[-2]where writing zero -length files would cause things to hang[-2].[sentence: 1,-2]  [result: max + and - of any sentence]
1	-1	DFS re-format should fully delete old namenode data.   	DFS re -format should fully delete old namenode data .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-102.  Contributed by Konstantin.    	Fix for HADOOP -102 .[sentence: 1,-1] Contributed by Konstantin .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix HADOOP-100.  Be more consistent about synchronization of access to taskTracker collection.  Contributed by Owen O'Malley.   	Fix HADOOP -100 .[sentence: 1,-1] Be more consistent about synchronization of access to taskTracker collection .[sentence: 1,-1] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-3	Fix for HADOOP-107.  As they were written, dfs blocks were both trickled to a datanode and tee'd to a temp file (in case the connection to the datanode failed).  Now they're only written to the temp file, with no connection to the datanode made until the block is complete.  This reduces the number of long-lived mostly-idle connections to datanodes, which was causing problems.  It also simplifies the DFSClient code significantly.   	Fix for HADOOP -107 .[sentence: 1,-1] As they were written ,dfs blocks[-2]were both trickled[-2]to a datanode and tee'd to a temp file (in case the connection to the datanode failed[-3]).[sentence: 1,-3] Now they're only written to the temp file ,with no connection to the datanode made until the block[-2]is complete .[sentence: 1,-2] This reduces the number of long -lived mostly -idle connections to datanodes ,which was causing problems[-2].[sentence: 1,-2] It also simplifies the DFSClient code significantly .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	Fix for HADOOP-103, part II: I forgot to add this file the first time!   	Fix for HADOOP -103 ,part II :I forgot to add this file the first time ![+0.6 punctuation mood emphasis][sentence: 2,-1]  [result: max + and - of any sentence]
2	-1	Fix for HADOOP-103.  Add a base class for Mapper and Reducer implementations that implements Closeable and JobConfigurable.  Use it in supplied Mappers & Reducers.  Also some minor improvements to demos.  Contributed by Owen O'Malley.   	Fix for HADOOP -103 .[sentence: 1,-1] Add a base class for Mapper and Reducer implementations that implements Closeable and JobConfigurable .[sentence: 1,-1] Use it in supplied Mappers &Reducers .[sentence: 1,-1] Also some minor improvements[2]to demos .[sentence: 2,-1] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	Fix for HADOOP-110.  Reuse keys and values when mapping.  Contributed by Owen O'Malley.    	Fix for HADOOP -110 .[sentence: 1,-1] Reuse keys and values[2]when mapping .[sentence: 2,-1] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	Fix for HADOOP-2.  The combiner now clones keys and values, so mappers may now safely reuse emitted keys and values.  Contributed by Owen O'Malley.   	Fix for HADOOP -2 .[sentence: 1,-1] The combiner now clones keys and values[2],so mappers may now safely[2]reuse emitted keys and values[2].[sentence: 2,-1] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Fix for HADOOP-112.  All operations on local files are now performed through a LocalFileSystem.  In particular, listing the local directory, which was causing this bug, when CRC files were included in the listing.  Now they are correctly excluded.    	Fix for HADOOP -112 .[sentence: 1,-1] All operations on local files are now performed through a LocalFileSystem .[sentence: 1,-1] In particular ,listing the local directory ,which was causing this bug[-2],when CRC files were included in the listing .[sentence: 1,-2] Now they are correctly excluded .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-2	Fix for HADOOP-84.  Improve error and log messages when block cannot be obtained by including file name and offset.  Also removed a few unused variables.  Contributed by Konstantin Shvachko.   	Fix for HADOOP -84 .[sentence: 1,-1] Improve[2]error[-2]and log messages when block[-2]cannot be obtained by including file name and offset .[sentence: 2,-2] Also removed a few unused variables .[sentence: 1,-1] Contributed by Konstantin Shvachko .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-2	Fix HADOOP-33.  Avoid calling df too frequently by caching values internally.  Contributed by Konstantin Shvachko.   	Fix HADOOP -33 .[sentence: 1,-1] Avoid[-2]calling df too frequently by caching values[2]internally .[sentence: 2,-2] Contributed by Konstantin Shvachko .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix HADOOP-67.  Add a public API for dfs statistics.  Also switch to use the public API for reporting in DFSShell.   	Fix HADOOP -67 .[sentence: 1,-1] Add a public API for dfs statistics .[sentence: 1,-1] Also switch to use the public API for reporting in DFSShell .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-2	Add a tool for checking DFS consistency (HADOOP-101).  Add a shortcut to bin/hadoop. In accordance with long-standing *nix tradition this command is called 'fsck'.  Development of this tool has been supported by Krugle.net. Thank you!    	Add a tool for checking DFS consistency (HADOOP -101 ).[sentence: 1,-1] Add a shortcut to bin /hadoop .[sentence: 1,-1] In accordance with long -standing *nix[-2]tradition this command is called 'fsck '.[sentence: 1,-2] Development of this tool has been supported[2]by Krugle .[sentence: 2,-1] net .[sentence: 1,-1] Thank[2]you ![+0.6 punctuation mood emphasis][sentence: 2,-1]  [result: max + and - of any sentence]
1	-1	Fix for file names with spaces.    	Fix for file names with spaces .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Always return an absolute pathname for local files.  This fixes problems on Windows, where a path specified with "/foo" in a config file is sometimes treated as a relative path.    	Always return an absolute pathname for local files .[sentence: 1,-1] This fixes problems[-2]on Windows ,where a path specified with '/foo 'in a config file is sometimes treated as a relative path .[sentence: 1,-2]  [result: max + and - of any sentence]
2	-2	Fix unit tests on Windows.  Don't assume that, just because a pathname begins with a slash that it also returns true for File.isAbsolute().  Instead use getAbsoluteFile() to force such things to be absolute.   	Fix unit tests on Windows .[sentence: 1,-1] Don't assume that ,just because a pathname begins with a slash[-2]that it also returns true[2]for File .[sentence: 2,-2] isAbsolute ().[sentence: 1,-1] Instead use getAbsoluteFile ()to force[-2]such things to be absolute .[sentence: 1,-2]  [result: max + and - of any sentence]
1	-1	Document the new format command.    	Document the new format command .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-3	Fix for HADOOP-19.  A namenode must now be formatted before it may be used.  Attempts to start a namenode in an unformatted directory will fail, rather than automatically creating a new, empty filesystem, causing existing datanodes to delete all blocks.  Thus a mis-configured dfs.data.dir should no longer cause data loss.   	Fix for HADOOP -19 .[sentence: 1,-1] A namenode must now be formatted before it may be used .[sentence: 1,-1] Attempts to start a namenode in an unformatted directory will fail[-3],rather than automatically creating a new ,empty filesystem ,causing existing datanodes to delete all blocks[-2].[sentence: 1,-3] Thus a mis -configured dfs .[sentence: 1,-1] data .[sentence: 1,-1] dir should no longer cause data loss[-3].[sentence: 1,-3]  [result: max + and - of any sentence]
1	-1	Move checking of output directory existence from JobClient to OutputFormat, so that it can be overridden.  Add a base class for OutputFormat that implements this new method.   	Move checking of output directory existence from JobClient to OutputFormat ,so that it can be overridden .[sentence: 1,-1] Add a base class for OutputFormat that implements this new method .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-46.  Jobs can now be named.  Contributed by Owen O'Malley.   	Fix for HADOOP -46 .[sentence: 1,-1] Jobs can now be named .[sentence: 1,-1] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-98.  Keep more accurate task counts.  Contributed by Owen O'Malley.   	Fix for HADOOP -98 .[sentence: 1,-1] Keep more accurate task counts .[sentence: 1,-1] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	Fix for HADOOP-52.  Add username and working-directory to FileSystem and JobConf and use these to resolve relative paths.  Contributed by Owen O'Malley.   	Fix for HADOOP -52 .[sentence: 1,-1] Add username and working -directory to FileSystem and JobConf and use these to resolve[2]relative paths .[sentence: 2,-1] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-83.   	Fix for HADOOP -83 .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Much improved hadoop logo.  Contributed by Stefan.  Thanks!   	Much improved[2]hadoop logo .[sentence: 2,-1] Contributed by Stefan .[sentence: 1,-1] Thanks[2]![+0.6 punctuation emphasis][sentence: 3,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-3.  Don't permit jobs to write to a pre-existing output directory.  Contributed by Owen O'Malley.   	Fix for HADOOP -3 .[sentence: 1,-1] Don't permit jobs to write to a pre -existing output directory .[sentence: 1,-1] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-3	Fix for HADOOP-45.  Fatal task errors are now logged at the JobTracker, facilitating debugging.    	Fix for HADOOP -45 .[sentence: 1,-1] Fatal[-3]task errors[-2]are now logged at the JobTracker ,facilitating debugging .[sentence: 1,-3]  [result: max + and - of any sentence]
1	-2	Fix for HADOOP-44.  The error string for remote exceptions now contains the full remote stack trace.  Remote exceptions are now also re-thrown on the client as RemoteException rather than IOException, so that they can be distinguished from other IOExceptions.   	Fix for HADOOP -44 .[sentence: 1,-1] The error[-2]string for remote exceptions now contains the full remote stack trace .[sentence: 1,-2] Remote exceptions are now also re -thrown on the client as RemoteException rather than IOException ,so that they can be distinguished from other IOExceptions .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Start namenode before datanodes to minimize datanode startup errors.    	Start namenode before datanodes to minimize datanode startup errors[-2].[sentence: 1,-2]  [result: max + and - of any sentence]
2	-2	Fix for HADOOP-97.  Improve error handling.  Contributed by Konstantin Shvachko.    	Fix for HADOOP -97 .[sentence: 1,-1] Improve[2]error[-2]handling .[sentence: 2,-2] Contributed by Konstantin Shvachko .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-87.  Dont' pass large buffers through to deflater as this is inefficient.    	Fix for HADOOP -87 .[sentence: 1,-1] Dont 'pass large buffers through to deflater as this is inefficient .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-93.  Convert min split size from int to long, and permit its specification in the config.   	Fix for HADOOP -93 .[sentence: 1,-1] Convert min split size from int to long ,and permit its specification in the config .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-3	Fix for HADOOP-86.  Errors while reading map output now cause map task to fail and be re-executed.   	Fix for HADOOP -86 .[sentence: 1,-1] Errors[-2]while reading map output now cause map task to fail[-3]and be re -executed[-2].[sentence: 1,-3]  [result: max + and - of any sentence]
3	-2	Give server implementations access to a server's context.  This consists of two additions.  First is a static method Server.get() which returns the server instance it is called under, if any.  Second is the new public class RPC.Server, that replaces a former anonymous class.  RPC server implementation methods can now subclass RPC.Server to keep server state in the subclass.  Application code can then call Server.get() to access that state.  Note that Server.get() may be called under parameter deserialization and return value serialization methods as well, called before and after actual server method calls, respectively.   	Give server implementations access to a server's context .[sentence: 1,-1] This consists of two additions .[sentence: 1,-1] First is a static[-2]method Server .[sentence: 1,-2] get ()which returns the server instance it is called under ,if any .[sentence: 1,-1] Second is the new public class RPC .[sentence: 1,-1] Server ,that replaces a former anonymous class .[sentence: 1,-1] RPC server implementation methods can now subclass RPC .[sentence: 1,-1] Server to keep server state in the subclass .[sentence: 1,-1] Application code can then call Server .[sentence: 1,-1] get ()to access that state .[sentence: 1,-1] Note that Server .[sentence: 1,-1] get ()may be called under parameter deserialization and return value[2]serialization methods as well ,called before and after actual server method calls ,respectively[3].[sentence: 3,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-82.  Completes count should never be less than zero.  Contributed by Michael Stack.   	Fix for HADOOP -82 .[sentence: 1,-1] Completes count should never be less than zero .[sentence: 1,-1] Contributed by Michael Stack .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-81.  Job-specific parameters should be read from the job-specific configuration, not the daemon's.  This permits speculative execution, number of map & reduce tasks, etc. to be settable in the job.  Contributed by Owen O'Malley.   	Fix for HADOOP -81 .[sentence: 1,-1] Job -specific parameters should be read from the job -specific configuration ,not the daemon's .[sentence: 1,-1] This permits speculative execution ,number of map &reduce tasks ,etc .[sentence: 1,-1] to be settable in the job .[sentence: 1,-1] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Fix for HADOOP-66.  Delete dfs temp files on JVM exit.   	Fix for HADOOP -66 .[sentence: 1,-1] Delete dfs temp files on JVM exit[-2].[sentence: 1,-2]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-80.  Make BytesWritable also a WritableComparable.  Also add hashBytes() utility method to WritableComparator and use it to hash both BytesWritable and UTF8.  Contributed by Owen O'Malley.   	Fix for HADOOP -80 .[sentence: 1,-1] Make BytesWritable also a WritableComparable .[sentence: 1,-1] Also add hashBytes ()utility method to WritableComparator and use it to hash both BytesWritable and UTF8 .[sentence: 1,-1] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Updated link to jira.   	Updated link to jira .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-79.  Some namenode optimizations.  Contributed by Konstantin Shvachko.   	Fix for HADOOP -79 .[sentence: 1,-1] Some namenode optimizations .[sentence: 1,-1] Contributed by Konstantin Shvachko .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Fix tasktracker to exit when errors are encountered reading map output, in order to force re-execution of map tasks.  It's overkill, since it will re-compute all map output computed by that task tracker, not just that which could not be read, but this should be a rare situation.  If we start seeing it frequently, then we could optimize this by adding a way to tell the jobtracker that a particular previously completed map task now needs to be re-executed.   	Fix tasktracker to exit[-2]when errors[-2]are encountered reading map output ,in order to force[-2]re -execution of map tasks .[sentence: 1,-2] It's overkill ,since it will re -compute all map output computed by that task tracker ,not just that which could not be read ,but this should be a rare situation .[sentence: 1,-1] If we start seeing it frequently ,then we could optimize this by adding a way to tell the jobtracker that a particular previously completed map task now needs to be re -executed[-2].[sentence: 1,-2]  [result: max + and - of any sentence]
1	-2	Fix for HADOOP-78.  Stream buffering was mistakenly disabled in writes by the RPC client.  Identified & fixed by Owen O'Malley.    	Fix for HADOOP -78 .[sentence: 1,-1] Stream buffering was mistakenly[-2]disabled[-2]in writes by the RPC client .[sentence: 1,-2] Identified &fixed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Use build/test for unit test data, instead of /tmp as in defaults.    	Use build /test for unit test data ,instead of /tmp as in defaults .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-77.  Fixes some NPEs.  Contributed by Stefan.   	Fix for HADOOP -77 .[sentence: 1,-1] Fixes some NPEs .[sentence: 1,-1] Contributed by Stefan .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Fix for HADOOP-70.  Unit tests should have their own hadoop-site.xml and mapred-default.xml, so that local modifications to these files in conf/ don't alter unit testing.  Also rename TestDFS so that it is not normally run, and add a new test target which runs tests using the config files in conf/.   	Fix for HADOOP -70 .[sentence: 1,-1] Unit tests should have their own hadoop -site .[sentence: 1,-1] xml and mapred -default[-2].[sentence: 1,-2] xml ,so that local modifications to these files in conf /don't alter unit testing .[sentence: 1,-1] Also rename TestDFS so that it is not normally run ,and add a new test target which runs tests using the config files in conf /.[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Reverted changes from 384385, which removed local backup copy of block & removed most timeouts.  That worked well when all hosts are healthy, but when a few are very slow it caused too many tasks to timeout and loads to balloon on slow hosts.  So the local backup is back, but no longer in /tmp, rather in dfs.data.dir, and timeouts are back.  I also added connect timeouts, so that dfs connects also don't get hung up by slow hosts.   	Reverted[-2]changes from 384385 ,which removed local backup copy of block[-2]&removed most timeouts .[sentence: 1,-2] That worked well when all hosts are healthy ,but when a few are very slow it caused too many tasks to timeout and loads to balloon on slow hosts .[sentence: 1,-1] So the local backup is back ,but no longer in /tmp ,rather in dfs .[sentence: 1,-1] data .[sentence: 1,-1] dir ,and timeouts are back .[sentence: 1,-1] I also added connect timeouts ,so that dfs connects also don't get hung up by slow hosts .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fixed wiki URL.   	Fixed wiki URL .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fixed wiki URL.   	Fixed wiki URL .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-3	Fix for HADOOP-66.  DFS blocks are no longer written to local temp files.  If a connection to a datanode fails then an exception is now thrown, rather than trying to re-connect to another datanode.  Timeouts were also removed from datanode connections, since these caused a lot of failed connections.   	Fix for HADOOP -66 .[sentence: 1,-1] DFS blocks[-2]are no longer written to local temp files .[sentence: 1,-2] If a connection to a datanode fails[-3]then an exception is now thrown ,rather than trying to re -connect to another datanode .[sentence: 1,-3] Timeouts were also removed from datanode connections ,since these caused a lot of failed[-3]connections .[sentence: 1,-3]  [result: max + and - of any sentence]
1	-1	Add ability to sleep a bit between each command that's submitted to a slave, to meter slave commands a bit.   	Add ability to sleep a bit between each command that's submitted to a slave ,to meter slave commands a bit .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-57.  Permit listing of "/" in dfs.  With help from Mahadev konar.    	Fix for HADOOP -57 .[sentence: 1,-1] Permit listing of '/'in dfs .[sentence: 1,-1] With help from Mahadev konar .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Apply local Configuration to parameters received via RPC if they implement Configurable. Fix by Marko Bauhardt.  Note: in the future there may be protocols that assume different local and remote Configuration. However, with the current RPC implementation we don't support it anyway, and for now it seems better that parameters implementing Configurable should be provided with non-null Configuration.    	Apply local Configuration to parameters received via RPC if they implement Configurable .[sentence: 1,-1] Fix by Marko Bauhardt .[sentence: 1,-1] Note :in the future there may be protocols that assume different local and remote Configuration .[sentence: 1,-1] However ,with the current RPC implementation we don't support[2][*-0.5 approx. negated multiplier]it anyway ,and for now it seems better that parameters implementing Configurable should be provided with non -null Configuration .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Increase deflater & inflater buffer size, for better performance.    	Increase deflater &inflater buffer size ,for better performance .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	Reduce iteration through all map & reduce tasks to improve jobtracker performance.    	Reduce iteration through all map &reduce tasks to improve[2]jobtracker performance .[sentence: 2,-1]  [result: max + and - of any sentence]
1	-1	Replace an NPE with an informative warning.    	Replace an NPE with an informative warning .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Increase some intervals to further reduce stress on the jobtracker.    	Increase some intervals to further reduce stress[-2]on the jobtracker .[sentence: 1,-2]  [result: max + and - of any sentence]
1	-1	Don't always query jobtracker for all needed map outputs, instead just for a random sample.  When the total number of splits was large, the jobtracker was spending most of its time servicing these requests. Also reduce the frequency of these requests.  Long-term we may need a different algorithm here to ensure that reduces are more promptly and efficiently notified of map completions.    	Don't always query jobtracker for all needed map outputs ,instead just for a random sample .[sentence: 1,-1] When the total number of splits was large ,the jobtracker was spending most of its time servicing these requests .[sentence: 1,-1] Also reduce the frequency of these requests .[sentence: 1,-1] Long -term we may need a different algorithm here to ensure that reduces are more promptly and efficiently notified of map completions .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-2	  The MapredLoadTest now has an extra step, to exercise the case where we have multiple reduce tasks.    It used to have two stages: one job that created a huge file of numbers in random order, followed by a job that would read that file and count the numbers.  If the final count was correct, the test passed.    Unfortunately, neither of these jobs had a reduce task that was greater than 1.    So now we've got three stages.  The first stage is unchanged.  The second stage reads the big file, then emits the answer key split into 10 parts, one for each reduce task.  then a third stage merges those parts into a final number count.  As before, if that final count is correct, all is well.     	The MapredLoadTest now has an extra step ,to exercise the case where we have multiple reduce tasks .[sentence: 1,-1] It used to have two stages :one job that created a huge file of numbers in random order ,followed by a job that would read that file and count the numbers .[sentence: 1,-1] If the final count was correct ,the test passed .[sentence: 1,-1] Unfortunately[-2],neither of these jobs had a reduce task that was greater[3]than 1 .[sentence: 3,-2] So now we've got three stages .[sentence: 1,-1] The first stage is unchanged .[sentence: 1,-1] The second stage reads the big file ,then emits the answer key split into 10 parts ,one for each reduce task .[sentence: 1,-1] then a third stage merges those parts into a final number count .[sentence: 1,-1] As before ,if that final count is correct ,all is well .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	  Fix bug HADOOP-26.  Available space is now considered correctly during DFS block-allocation.      	Fix bug[-2]HADOOP -26 .[sentence: 1,-2] Available space is now considered correctly during DFS block[-2]-allocation .[sentence: 1,-2]  [result: max + and - of any sentence]
1	-2	  This code makes sure we read from a local block, if available.  I thought this code had already been committed some time ago, but the workspace doesn't have it.     	This code makes sure we read from a local block[-2],if available .[sentence: 1,-2] I thought this code had already been committed some time ago ,but the workspace doesn't have it .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix a couple of tasktracker bugs.   	Fix a couple of tasktracker bugs .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	  A bug in the TaskTracker was governing task-allocation by counting the total number of tasks.  The right thing to do is keep a total for map tasks, and a separate total for reduces.    This is now fixed.     	A bug[-2]in the TaskTracker was governing task -allocation by counting the total number of tasks .[sentence: 1,-2] The right thing to do is keep a total for map tasks ,and a separate total for reduces .[sentence: 1,-1] This is now fixed .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Check taskid's more carefully.  Suggested by Michael Stack.    	Check taskid's more carefully .[sentence: 1,-1] Suggested by Michael Stack .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-16.  Splitting and other job planning is now performed in a separate thread.   	Fix for HADOOP -16 .[sentence: 1,-1] Splitting and other job planning is now performed in a separate thread .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Upgrade to Lucene 1.9.1.   	Upgrade to Lucene 1 .[sentence: 1,-1] 9 .[sentence: 1,-1] 1 .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-60, with help from Owen & Michael.   	Fix for HADOOP -60 ,with help from Owen &Michael .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Permit folks to modify options passed to ssh.  For example, older versions of ssh do not support the ConnectTimeout option.   	Permit folks to modify options passed to ssh .[sentence: 1,-1] For example ,older versions of ssh do not support[2][*-0.5 approx. negated multiplier]the ConnectTimeout option .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Add webapps to classpath so they're found.    	Add webapps to classpath so they're found .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix rsync command to (a) use ssh as transport, and (b) to correctly quote the path.    	Fix rsync command to (a )use ssh as transport ,and (b )to correctly quote the path .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix to make independent of "unzip", using java's built in unzip code instead.    	Fix to make independent of 'unzip ',using java's built in unzip code instead .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	Fix command line (again!).    	Fix command line (again !).[+0.6 punctuation mood emphasis][sentence: 2,-1]  [result: max + and - of any sentence]
1	-1	Update example command line.    	Update example command line .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Un-escape containing jar's path, which is URL-encoded.  This fixes things primarily on Windows, where paths are likely to contain spaces.    	Un -escape containing jar's path ,which is URL -encoded .[sentence: 1,-1] This fixes things primarily on Windows ,where paths are likely to contain spaces .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Escape paths so that spaces are permitted (as is common on Windows.)   	Escape paths so that spaces are permitted (as is common on Windows .)[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Upgrade lucene version to final release.   	Upgrade lucene version to final release .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	Minor improvements to DOAP file.    	Minor improvements[2]to DOAP file .[sentence: 2,-1]  [result: max + and - of any sentence]
1	-1	Add a DOAP description for hadoop.   	Add a DOAP description for hadoop .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-3	Print stack trace when child fails to contact parent.    	Print stack trace when child fails[-3]to contact parent .[sentence: 1,-3]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-40.  Buffer size was ignored.   	Fix for HADOOP -40 .[sentence: 1,-1] Buffer size was ignored .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	Fix for HADOOP-41.  Support passing more options to child JVM.  Contributed by Michael Stack.   	Fix for HADOOP -41 .[sentence: 1,-1] Support[2]passing more options to child JVM .[sentence: 2,-1] Contributed by Michael Stack .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-40.  Buffer position was not maintained correctly.  Contributed by Konstantin Shvachko.   	Fix for HADOOP -40 .[sentence: 1,-1] Buffer position was not maintained correctly .[sentence: 1,-1] Contributed by Konstantin Shvachko .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-49: Permit specification of jobtracker when submitting jobs.   	Fix for HADOOP -49 :Permit specification of jobtracker when submitting jobs .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Fix so that task state is displayed even when there are no errors.  Also changed report to be a datastructure rather than a vector of strings.   	Fix so that task state is displayed even when there are no errors[-2].[sentence: 1,-2] Also changed report to be a datastructure rather than a vector of strings .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Updated javadoc for recent config changes.   	Updated javadoc for recent config changes .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Add missing synchronization.   	Add missing synchronization .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	Make speculative execution optional, since it can break map tasks that have side effects.  Also fix TestFileSystem to be safe to use with speculative execution.   	Make speculative execution optional ,since it can break map tasks that have side effects .[sentence: 1,-1] Also fix TestFileSystem to be safe[2]to use with speculative execution .[sentence: 2,-1]  [result: max + and - of any sentence]
1	-2	Fix HADOOP-36.  Scripts now source conf/hadoop-env.sh, to faciliate setting of environment variables, even on remote hosts.  The default slaves file has move from ~/.slaves to conf/slaves.   	Fix HADOOP -36 .[sentence: 1,-1] Scripts now source[-2]conf /hadoop -env .[sentence: 1,-2] sh ,to faciliate setting of environment variables ,even on remote hosts .[sentence: 1,-1] The default[-2]slaves file has move from ~/.[sentence: 1,-2] slaves to conf /slaves .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	Fix HADOOP-38: Add FileSystem.getBlockSize() method and use it as the maximum split size.  Also change FileSystem to implement Configurable, and improve some javadoc, using inherited comments where possible and removing implementation details from public javadoc.   	Fix HADOOP -38 :Add FileSystem .[sentence: 1,-1] getBlockSize ()method and use it as the maximum split size .[sentence: 1,-1] Also change FileSystem to implement Configurable ,and improve[2]some javadoc ,using inherited comments where possible and removing implementation details from public javadoc .[sentence: 2,-1]  [result: max + and - of any sentence]
1	-1	Move Closeable interface to io package, since it is of general utility, and to prepare for JDK 1.5.   	Move Closeable interface to io package ,since it is of general utility ,and to prepare for JDK 1 .[sentence: 1,-1] 5 .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	  Fix bug HADOOP-16.    Don't invoke TaskInProgress.hasTaskWithCacheHit() unnecessarily from within JobInProgress.    Also, cache filesystem hints inside JobInProgress.     	Fix bug[-2]HADOOP -16 .[sentence: 1,-2] Don't invoke TaskInProgress .[sentence: 1,-1] hasTaskWithCacheHit ()unnecessarily[-2]from within JobInProgress .[sentence: 1,-2] Also ,cache filesystem hints inside JobInProgress .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	HADOOP-37: Add ClusterStatus.  Contributed by Owen O'Malley.   	HADOOP -37 :Add ClusterStatus .[sentence: 1,-1] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-34: make build paths relative to location of build.xml, not PWD.  Contributed by Jeremy Bensley.   	Fix for HADOOP -34 :make build paths relative to location of build .[sentence: 1,-1] xml ,not PWD .[sentence: 1,-1] Contributed by Jeremy Bensley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	  Add a bunch of updated comments and JavaDocs to the Distributed File System package.     	Add a bunch of updated comments and JavaDocs to the Distributed File System package .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Bundle webapps into jar.   	Bundle webapps into jar .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix HADOOP-30: add -lsr and -cat commands to DFSShell.  Contributed by Michel Tourn.   	Fix HADOOP -30 :add -lsr and -cat commands to DFSShell .[sentence: 1,-1] Contributed by Michel Tourn .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	Further improvements from Bryan A. Pendleton.    	Further improvements[2]from Bryan A .[sentence: 2,-1] Pendleton .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Make compatible with JDK 1.4.    	Make compatible with JDK 1 .[sentence: 1,-1] 4 .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-12.  The JobTracker now loads the InputFormat from the job's jar file.   	Fix for HADOOP -12 .[sentence: 1,-1] The JobTracker now loads the InputFormat from the job's jar file .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Keep 'unzip' from prompting when overwriting (e.g., when archive contains same file twice).  Also make it less verbose.    	Keep 'unzip 'from prompting when overwriting (e .[sentence: 1,-1] g .,[sentence: 1,-1] when archive contains same file twice ).[sentence: 1,-1] Also make it less verbose .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Add an easy way to specify a job's jar, by naming a class in the jar.   	Add an easy way to specify a job's jar ,by naming a class in the jar .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Stop using deprecated 'jspc' ant task and instead call jasper directly.  Also rename webapp files to align with package name.   	Stop using deprecated 'jspc 'ant task and instead call jasper directly .[sentence: 1,-1] Also rename webapp files to align with package name .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fixed HADOOP-20: permit mappers and reducers to cleanup.  Add a close() method to the Mapper and Reducer interfaces by having them extend a Closeable interface.  Update all implementations to define close().  Patch by Michel Tourn.   	Fixed HADOOP -20 :permit mappers and reducers to cleanup .[sentence: 1,-1] Add a close ()method to the Mapper and Reducer interfaces by having them extend a Closeable interface .[sentence: 1,-1] Update all implementations to define close ().[sentence: 1,-1] Patch by Michel Tourn .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Removed properties mistakenly copied from Nutch.    	Removed properties mistakenly[-2]copied from Nutch .[sentence: 1,-2]  [result: max + and - of any sentence]
1	-1	Create html documentation for configuration defaults and link to these from javadoc.   	Create html documentation for configuration defaults and link to these from javadoc .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for split from nutch.    	Fix for split from nutch .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fixed so that examples jar is packaged correctly.    	Fixed so that examples jar is packaged correctly .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix HADOOP-28.  Jsp pages are now pre-compiled to servlets that can access package-private classes.   	Fix HADOOP -28 .[sentence: 1,-1] Jsp pages are now pre -compiled to servlets that can access package -private classes .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	Fix HADOOP-25: improve example code & package separately.  Contributed by Owen O'Malley.   	Fix HADOOP -25 :improve[2]example code &package separately .[sentence: 2,-1] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Updated javadoc.    	Updated javadoc .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Initial version of a Hadoop tutorial.    	Initial version of a Hadoop tutorial .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Clean up javadoc, reduce number of public classes.   	Clean up javadoc ,reduce number of public classes .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Log to logs directory by default.    	Log to logs directory by default[-2].[sentence: 1,-2]  [result: max + and - of any sentence]
1	-1	Remove a spurious space.    	Remove a spurious space .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Add WritableFactory mechanism, to permit one to use ObjectWritable with non-public classes.  Register factories for DFS implementation classes and make them non-public.  This greatly simplifies the javadoc.   	Add WritableFactory mechanism ,to permit one to use ObjectWritable with non -public classes .[sentence: 1,-1] Register factories for DFS implementation classes and make them non -public .[sentence: 1,-1] This greatly[3]simplifies the javadoc .[sentence: 3,-1]  [result: max + and - of any sentence]
2	-1	Improved javadoc, starting overview and package documentation.  Also moved DF from dfs to fs package.   	Improved[2]javadoc ,starting overview and package documentation .[sentence: 2,-1] Also moved DF from dfs to fs package .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Add scripts into jar file so they're bundled with code.   	Add scripts into jar file so they're bundled with code .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fixed javadoc link and added more news.   	Fixed javadoc link and added more news .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-22: remove unused imports.  By Sami Siren.   	Fix for HADOOP -22 :remove unused imports .[sentence: 1,-1] By Sami Siren .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix HADOOP-21: fix mapred webapp jsp for split from Nutch.  Contributed by Owen O'Malley.   	Fix HADOOP -21 :fix mapred webapp jsp for split from Nutch .[sentence: 1,-1] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Remove vestige of Nutch's build.xml so that nightly target will run.    	Remove vestige of Nutch's build .[sentence: 1,-1] xml so that nightly target will run .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix HADOOP-5: add commons logging jar to lib.   	Fix HADOOP -5 :add commons logging jar to lib .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-2	Remove a few vestiges of Nutch's plugin mechanism, fixing bug HADOOP-6 (Thanks, Owen!).  Also fix a few typos.    	Remove a few vestiges of Nutch's plugin mechanism ,fixing bug[-2]HADOOP -6 (Thanks[2],Owen !).[+0.6 punctuation mood emphasis][sentence: 2,-2] Also fix a few typos .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Fix some config problems, remove notion of app resources: it's overkill.   	Fix some config problems[-2],remove notion of app resources :it's overkill .[sentence: 1,-2]  [result: max + and - of any sentence]
1	-1	Don't require final resoureces.   	Don't require final resoureces .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Permit multiple default and final Configuration resources.   	Permit multiple default[-2]and final Configuration resources .[sentence: 1,-2]  [result: max + and - of any sentence]
1	-1	First version that passes unit tests.   	First version that passes unit tests .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	First version that compiles.   	First version that compiles .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Initial commit of code copied from Nutch.   	Initial commit of code copied from Nutch .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Initial commit of code copied from Nutch.   	Initial commit of code copied from Nutch .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Slightly improved logo.   	Slightly improved[2][+-1 booster word]logo .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	First version of website.   	First version of website .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Create hadoop sub-project.   	Create hadoop sub -project .[sentence: 1,-1]  [result: max + and - of any sentence]
